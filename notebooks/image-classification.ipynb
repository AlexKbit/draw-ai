{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import random\n",
    "from io import BytesIO\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "from os import path\n",
    "import time\n",
    "import argparse\n",
    "import wget\n",
    "import ast\n",
    "import json\n",
    "from PIL import Image, ImageDraw, ImageOps\n",
    "import ndjson\n",
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.pyplot as plt\n",
    "import image_utils\n",
    "from image_utils import add_flipped_and_rotated_images\n",
    "\n",
    "# import pytorch\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# import machine learning libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['apple', 'clock', 'door', 'diamond', 'fork', 'eye', 'star', 'axe', 'sword', 'fish']\n",
    "data_filepath = '../datasets'\n",
    "num_categories = len(categories)\n",
    "num_examples = 1000\n",
    "label_dict = {}\n",
    "width = 256\n",
    "height = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,num_categories):\n",
    "    label_dict[i] = categories[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save label dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../service/label_dict.json', 'w') as f:\n",
    "    json.dump(categories, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google quickdraw dataset (<a href='https://github.com/googlecreativelab/quickdraw-dataset#the-raw-moderated-dataset'>More</a>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL to dataset in GCP Storage\n",
    "dataset_url = 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset for apple is already downloaded.\n",
      "Dataset for clock is already downloaded.\n",
      "Dataset for door is already downloaded.\n",
      "Dataset for diamond is already downloaded.\n",
      "Dataset for fork is already downloaded.\n",
      "Dataset for eye is already downloaded.\n",
      "Dataset for star is already downloaded.\n",
      "Dataset for axe is already downloaded.\n",
      "Dataset for sword is already downloaded.\n",
      "Dataset for fish is already downloaded.\n"
     ]
    }
   ],
   "source": [
    "for category in categories:\n",
    "    if not os.path.exists(data_filepath + '/' + str(category) + '.npy'):\n",
    "        print(\"Start downloading data process for [{}].\".format(category))\n",
    "        url = dataset_url + str(category) + '.npy'\n",
    "        wget.download(\n",
    "                    url=url,\n",
    "                    out=data_filepath\n",
    "                )\n",
    "        print(\"Dataset for {} was successfully downloaded.\".format(category))\n",
    "    else:\n",
    "        print(\"Dataset for {} is already downloaded.\".format(category))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_dict = {}\n",
    "for category in categories:\n",
    "    classes_dict[category] = np.load(data_filepath + '/' + str(category) + '.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate labels and add labels to loaded data\n",
    "for i, (key, value) in enumerate(classes_dict.items()):\n",
    "    value = value.astype('float32')/255.\n",
    "    if i == 0:\n",
    "        classes_dict[key] = np.c_[value, np.zeros(len(value))]\n",
    "    else:\n",
    "        classes_dict[key] = np.c_[value,i*np.ones(len(value))]\n",
    "\n",
    "lst = []\n",
    "for key, value in classes_dict.items():\n",
    "    lst.append(value[:3000])\n",
    "tmp = np.concatenate(lst)\n",
    "\n",
    "# Split the data into features and class labels (X & y respectively)\n",
    "y = tmp[:,-1].astype('float32')\n",
    "X = tmp[:,:784]\n",
    "\n",
    "# Split each dataset into train/test splits\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_images_grid(X, y):\n",
    "    fig, axs = plt.subplots(5, 10, figsize=(20,10))\n",
    "    \n",
    "    for label_num in range(0,50):\n",
    "        r_label = random.randint(0, len(X) - 1)\n",
    "        image = X[r_label].reshape(28,28)  #reshape images\n",
    "        i = label_num // 10\n",
    "        j = label_num % 10\n",
    "        axs[i,j].imshow(image) #plot the data\n",
    "        axs[i,j].axis('off')\n",
    "        axs[i,j].set_title(label_dict[y[r_label]])\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding flipped and rotated images to the training set. \n",
      "\n",
      "Processed 0 files out of 21000.\n",
      "Processed 100 files out of 21000.\n",
      "Processed 200 files out of 21000.\n",
      "Processed 300 files out of 21000.\n",
      "Processed 400 files out of 21000.\n",
      "Processed 500 files out of 21000.\n",
      "Processed 600 files out of 21000.\n",
      "Processed 700 files out of 21000.\n",
      "Processed 800 files out of 21000.\n",
      "Processed 900 files out of 21000.\n",
      "Processed 1000 files out of 21000.\n",
      "Processed 1100 files out of 21000.\n",
      "Processed 1200 files out of 21000.\n",
      "Processed 1300 files out of 21000.\n",
      "Processed 1400 files out of 21000.\n",
      "Processed 1500 files out of 21000.\n",
      "Processed 1600 files out of 21000.\n",
      "Processed 1700 files out of 21000.\n",
      "Processed 1800 files out of 21000.\n",
      "Processed 1900 files out of 21000.\n",
      "Processed 2000 files out of 21000.\n",
      "Processed 2100 files out of 21000.\n",
      "Processed 2200 files out of 21000.\n",
      "Processed 2300 files out of 21000.\n",
      "Processed 2400 files out of 21000.\n",
      "Processed 2500 files out of 21000.\n",
      "Processed 2600 files out of 21000.\n",
      "Processed 2700 files out of 21000.\n",
      "Processed 2800 files out of 21000.\n",
      "Processed 2900 files out of 21000.\n",
      "Processed 3000 files out of 21000.\n",
      "Processed 3100 files out of 21000.\n",
      "Processed 3200 files out of 21000.\n",
      "Processed 3300 files out of 21000.\n",
      "Processed 3400 files out of 21000.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-428bbb851236>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_flipped_and_rotated_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/Develop/Projects/draw-ai/notebooks/image_utils.py\u001b[0m in \u001b[0;36madd_flipped_and_rotated_images\u001b[0;34m(X_train, y_train)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;31m# add to the original dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0mX_train_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrotated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m784\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0mX_train_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflipped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m784\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m         \u001b[0my_train_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0my_train_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mappend\u001b[0;34m(arr, values, axis)\u001b[0m\n\u001b[1;32m   4692\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4693\u001b[0m         \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4694\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_train, y_train = image_utils.add_flipped_and_rotated_images(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_images_grid(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tensor_train = torch.from_numpy(X_train).float()\n",
    "y_tensor_train = torch.from_numpy(y_train).long()\n",
    "X_tensor_test = torch.from_numpy(X_test).float()\n",
    "y_tensor_test = torch.from_numpy(y_test).long()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for our network\n",
    "input_size = 784\n",
    "hidden_sizes = [128, 100, 64]\n",
    "output_size = 10\n",
    "dropout = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_size = 784, output_size = 10, hidden_sizes = [128, 100, 64]):\n",
    "    return nn.Sequential(OrderedDict([\n",
    "                            ('fc1', nn.Linear(input_size, hidden_sizes[0])),\n",
    "                            ('relu1', nn.ReLU()),\n",
    "                            ('fc2', nn.Linear(hidden_sizes[0], hidden_sizes[1])),\n",
    "                            ('bn2', nn.BatchNorm1d(num_features = hidden_sizes[1])),\n",
    "                            ('relu2', nn.ReLU()),\n",
    "                            ('dropout', nn.Dropout(dropout)),\n",
    "                            ('fc3', nn.Linear(hidden_sizes[1], hidden_sizes[2])),\n",
    "                            ('bn3', nn.BatchNorm1d(num_features = hidden_sizes[2])),\n",
    "                            ('relu3', nn.ReLU()),\n",
    "                            ('logits', nn.Linear(hidden_sizes[2], output_size))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = X_tensor_train\n",
    "train_labels = y_tensor_train\n",
    "test = X_tensor_test\n",
    "test_labels = y_tensor_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(X_train, y_train):\n",
    "    X_train_shuffled = X_train.numpy()\n",
    "    y_train_shuffled = y_train.numpy().reshape((X_train.shape[0], 1))\n",
    "\n",
    "    permutation = list(np.random.permutation(X_train.shape[0]))\n",
    "    X_train_shuffled = X_train_shuffled[permutation, :]\n",
    "    y_train_shuffled = y_train_shuffled[permutation, :].reshape((X_train.shape[0], 1))\n",
    "\n",
    "    X_train_shuffled = torch.from_numpy(X_train_shuffled).float()\n",
    "    y_train_shuffled = torch.from_numpy(y_train_shuffled).long()\n",
    "\n",
    "    return X_train_shuffled, y_train_shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(model, X_train, y_train, epochs = 100, n_chunks = 1000, learning_rate = 0.003, weight_decay = 0):\n",
    "    print(\"Fitting model with epochs = {epochs}, learning rate = {lr}\"\\\n",
    "    .format(epochs = epochs, lr = learning_rate))\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay= weight_decay)\n",
    "\n",
    "    print_every = 100\n",
    "    steps = 0\n",
    "\n",
    "    for e in range(epochs):\n",
    "        running_loss = 0\n",
    "\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "\n",
    "        images = torch.chunk(X_train, n_chunks)\n",
    "        labels = torch.chunk(y_train, n_chunks)\n",
    "\n",
    "        for i in range(n_chunks):\n",
    "            steps += 1\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward and backward passes\n",
    "            output = model.forward(images[i])\n",
    "            loss = criterion(output, labels[i].squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if steps % print_every == 0:\n",
    "                print(\"Epoch: {}/{} \".format(e+1, epochs),\n",
    "                      \"Loss: {:.4f}\".format(running_loss/print_every))\n",
    "\n",
    "                running_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds(model, input):\n",
    "    with torch.no_grad():\n",
    "        logits = model.forward(input)\n",
    "\n",
    "    ps = F.softmax(logits, dim=1)\n",
    "    return ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(pred):\n",
    "    pred_np = pred.numpy()\n",
    "    pred_values = np.amax(pred_np, axis=1, keepdims=True)\n",
    "    pred_labels = np.array([np.where(pred_np[i, :] == pred_values[i, :])[0] for i in range(pred_np.shape[0])])\n",
    "    pred_labels = pred_labels.reshape(len(pred_np), 1)\n",
    "\n",
    "    return pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train, y_train, test, y_test):\n",
    "    train_pred = get_preds(model, train)\n",
    "    train_pred_labels = get_labels(train_pred)\n",
    "\n",
    "    test_pred = get_preds(model, test)\n",
    "    test_pred_labels = get_labels(test_pred)\n",
    "\n",
    "    accuracy_train = accuracy_score(y_train, train_pred_labels)\n",
    "    accuracy_test = accuracy_score(y_test, test_pred_labels)\n",
    "\n",
    "    print(\"Accuracy score for train set is {}\".format(accuracy_train))\n",
    "    print(\"Accuracy score for test set is {}\".format(accuracy_test))\n",
    "    return accuracy_train, accuracy_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot learning curve depending on the number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.003\n",
    "weight_decay = 0.0\n",
    "n_chunks = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model with epochs = 10, learning rate = 0.003\n",
      "Epoch: 1/10  Loss: 1.0617\n",
      "Epoch: 1/10  Loss: 0.8221\n",
      "Epoch: 1/10  Loss: 0.6735\n",
      "Epoch: 1/10  Loss: 0.6441\n",
      "Epoch: 1/10  Loss: 0.6412\n",
      "Epoch: 1/10  Loss: 0.6139\n",
      "Epoch: 1/10  Loss: 0.6330\n",
      "Epoch: 1/10  Loss: 0.6121\n",
      "Epoch: 1/10  Loss: 0.5766\n",
      "Epoch: 1/10  Loss: 0.5755\n",
      "Epoch: 2/10  Loss: 0.5411\n",
      "Epoch: 2/10  Loss: 0.5133\n",
      "Epoch: 2/10  Loss: 0.4826\n",
      "Epoch: 2/10  Loss: 0.5211\n",
      "Epoch: 2/10  Loss: 0.4961\n",
      "Epoch: 2/10  Loss: 0.5214\n",
      "Epoch: 2/10  Loss: 0.4856\n",
      "Epoch: 2/10  Loss: 0.4497\n",
      "Epoch: 2/10  Loss: 0.4754\n",
      "Epoch: 2/10  Loss: 0.4592\n",
      "Epoch: 3/10  Loss: 0.3761\n",
      "Epoch: 3/10  Loss: 0.4518\n",
      "Epoch: 3/10  Loss: 0.4348\n",
      "Epoch: 3/10  Loss: 0.4275\n",
      "Epoch: 3/10  Loss: 0.4580\n",
      "Epoch: 3/10  Loss: 0.4234\n",
      "Epoch: 3/10  Loss: 0.4293\n",
      "Epoch: 3/10  Loss: 0.4081\n",
      "Epoch: 3/10  Loss: 0.4439\n",
      "Epoch: 3/10  Loss: 0.4186\n",
      "Epoch: 4/10  Loss: 0.3868\n",
      "Epoch: 4/10  Loss: 0.3854\n",
      "Epoch: 4/10  Loss: 0.3503\n",
      "Epoch: 4/10  Loss: 0.3716\n",
      "Epoch: 4/10  Loss: 0.3928\n",
      "Epoch: 4/10  Loss: 0.3531\n",
      "Epoch: 4/10  Loss: 0.3622\n",
      "Epoch: 4/10  Loss: 0.4062\n",
      "Epoch: 4/10  Loss: 0.3833\n",
      "Epoch: 4/10  Loss: 0.3894\n",
      "Epoch: 5/10  Loss: 0.2871\n",
      "Epoch: 5/10  Loss: 0.3281\n",
      "Epoch: 5/10  Loss: 0.3464\n",
      "Epoch: 5/10  Loss: 0.3049\n",
      "Epoch: 5/10  Loss: 0.3300\n",
      "Epoch: 5/10  Loss: 0.3237\n",
      "Epoch: 5/10  Loss: 0.3565\n",
      "Epoch: 5/10  Loss: 0.3414\n",
      "Epoch: 5/10  Loss: 0.3704\n",
      "Epoch: 5/10  Loss: 0.3516\n",
      "Epoch: 6/10  Loss: 0.2501\n",
      "Epoch: 6/10  Loss: 0.3013\n",
      "Epoch: 6/10  Loss: 0.3077\n",
      "Epoch: 6/10  Loss: 0.2946\n",
      "Epoch: 6/10  Loss: 0.2911\n",
      "Epoch: 6/10  Loss: 0.3058\n",
      "Epoch: 6/10  Loss: 0.2726\n",
      "Epoch: 6/10  Loss: 0.3110\n",
      "Epoch: 6/10  Loss: 0.3240\n",
      "Epoch: 6/10  Loss: 0.2835\n",
      "Epoch: 7/10  Loss: 0.2736\n",
      "Epoch: 7/10  Loss: 0.2114\n",
      "Epoch: 7/10  Loss: 0.2663\n",
      "Epoch: 7/10  Loss: 0.2670\n",
      "Epoch: 7/10  Loss: 0.2776\n",
      "Epoch: 7/10  Loss: 0.2896\n",
      "Epoch: 7/10  Loss: 0.2457\n",
      "Epoch: 7/10  Loss: 0.2974\n",
      "Epoch: 7/10  Loss: 0.2576\n",
      "Epoch: 7/10  Loss: 0.2937\n",
      "Epoch: 8/10  Loss: 0.2189\n",
      "Epoch: 8/10  Loss: 0.2091\n",
      "Epoch: 8/10  Loss: 0.2226\n",
      "Epoch: 8/10  Loss: 0.2363\n",
      "Epoch: 8/10  Loss: 0.2557\n",
      "Epoch: 8/10  Loss: 0.2816\n",
      "Epoch: 8/10  Loss: 0.2800\n",
      "Epoch: 8/10  Loss: 0.2633\n",
      "Epoch: 8/10  Loss: 0.2535\n",
      "Epoch: 8/10  Loss: 0.2494\n",
      "Epoch: 9/10  Loss: 0.1905\n",
      "Epoch: 9/10  Loss: 0.1848\n",
      "Epoch: 9/10  Loss: 0.2082\n",
      "Epoch: 9/10  Loss: 0.2317\n",
      "Epoch: 9/10  Loss: 0.2131\n",
      "Epoch: 9/10  Loss: 0.2001\n",
      "Epoch: 9/10  Loss: 0.2581\n",
      "Epoch: 9/10  Loss: 0.2550\n",
      "Epoch: 9/10  Loss: 0.2513\n",
      "Epoch: 9/10  Loss: 0.2553\n",
      "Epoch: 10/10  Loss: 0.1657\n",
      "Epoch: 10/10  Loss: 0.1502\n",
      "Epoch: 10/10  Loss: 0.1804\n",
      "Epoch: 10/10  Loss: 0.1763\n",
      "Epoch: 10/10  Loss: 0.2111\n",
      "Epoch: 10/10  Loss: 0.2210\n",
      "Epoch: 10/10  Loss: 0.2372\n",
      "Epoch: 10/10  Loss: 0.2116\n",
      "Epoch: 10/10  Loss: 0.2348\n",
      "Epoch: 10/10  Loss: 0.1980\n",
      "Accuracy score for train set is 0.961952380952381\n",
      "Accuracy score for test set is 0.8803333333333333\n",
      "Fitting model with epochs = 20, learning rate = 0.003\n",
      "Epoch: 1/20  Loss: 1.0470\n",
      "Epoch: 1/20  Loss: 0.7708\n",
      "Epoch: 1/20  Loss: 0.7069\n",
      "Epoch: 1/20  Loss: 0.7044\n",
      "Epoch: 1/20  Loss: 0.6429\n",
      "Epoch: 1/20  Loss: 0.6334\n",
      "Epoch: 1/20  Loss: 0.6301\n",
      "Epoch: 1/20  Loss: 0.6015\n",
      "Epoch: 1/20  Loss: 0.5693\n",
      "Epoch: 1/20  Loss: 0.5481\n",
      "Epoch: 2/20  Loss: 0.4820\n",
      "Epoch: 2/20  Loss: 0.5230\n",
      "Epoch: 2/20  Loss: 0.4890\n",
      "Epoch: 2/20  Loss: 0.5149\n",
      "Epoch: 2/20  Loss: 0.5364\n",
      "Epoch: 2/20  Loss: 0.4982\n",
      "Epoch: 2/20  Loss: 0.4577\n",
      "Epoch: 2/20  Loss: 0.4832\n",
      "Epoch: 2/20  Loss: 0.5130\n",
      "Epoch: 2/20  Loss: 0.4581\n",
      "Epoch: 3/20  Loss: 0.3932\n",
      "Epoch: 3/20  Loss: 0.4274\n",
      "Epoch: 3/20  Loss: 0.4092\n",
      "Epoch: 3/20  Loss: 0.4496\n",
      "Epoch: 3/20  Loss: 0.4396\n",
      "Epoch: 3/20  Loss: 0.4089\n",
      "Epoch: 3/20  Loss: 0.4459\n",
      "Epoch: 3/20  Loss: 0.3762\n",
      "Epoch: 3/20  Loss: 0.4881\n",
      "Epoch: 3/20  Loss: 0.4315\n",
      "Epoch: 4/20  Loss: 0.3527\n",
      "Epoch: 4/20  Loss: 0.3797\n",
      "Epoch: 4/20  Loss: 0.3891\n",
      "Epoch: 4/20  Loss: 0.3226\n",
      "Epoch: 4/20  Loss: 0.3607\n",
      "Epoch: 4/20  Loss: 0.3695\n",
      "Epoch: 4/20  Loss: 0.3749\n",
      "Epoch: 4/20  Loss: 0.3735\n",
      "Epoch: 4/20  Loss: 0.3695\n",
      "Epoch: 4/20  Loss: 0.3594\n",
      "Epoch: 5/20  Loss: 0.3495\n",
      "Epoch: 5/20  Loss: 0.2986\n",
      "Epoch: 5/20  Loss: 0.3595\n",
      "Epoch: 5/20  Loss: 0.3545\n",
      "Epoch: 5/20  Loss: 0.3171\n",
      "Epoch: 5/20  Loss: 0.3228\n",
      "Epoch: 5/20  Loss: 0.3323\n",
      "Epoch: 5/20  Loss: 0.3311\n",
      "Epoch: 5/20  Loss: 0.3357\n",
      "Epoch: 5/20  Loss: 0.3471\n",
      "Epoch: 6/20  Loss: 0.2579\n",
      "Epoch: 6/20  Loss: 0.2647\n",
      "Epoch: 6/20  Loss: 0.2921\n",
      "Epoch: 6/20  Loss: 0.2804\n",
      "Epoch: 6/20  Loss: 0.2907\n",
      "Epoch: 6/20  Loss: 0.2928\n",
      "Epoch: 6/20  Loss: 0.2914\n",
      "Epoch: 6/20  Loss: 0.3327\n",
      "Epoch: 6/20  Loss: 0.3409\n",
      "Epoch: 6/20  Loss: 0.3376\n",
      "Epoch: 7/20  Loss: 0.2588\n",
      "Epoch: 7/20  Loss: 0.2289\n",
      "Epoch: 7/20  Loss: 0.2603\n",
      "Epoch: 7/20  Loss: 0.2477\n",
      "Epoch: 7/20  Loss: 0.2659\n",
      "Epoch: 7/20  Loss: 0.2572\n",
      "Epoch: 7/20  Loss: 0.2650\n",
      "Epoch: 7/20  Loss: 0.2927\n",
      "Epoch: 7/20  Loss: 0.2666\n",
      "Epoch: 7/20  Loss: 0.2633\n",
      "Epoch: 8/20  Loss: 0.2107\n",
      "Epoch: 8/20  Loss: 0.2047\n",
      "Epoch: 8/20  Loss: 0.2074\n",
      "Epoch: 8/20  Loss: 0.2105\n",
      "Epoch: 8/20  Loss: 0.2673\n",
      "Epoch: 8/20  Loss: 0.2451\n",
      "Epoch: 8/20  Loss: 0.2557\n",
      "Epoch: 8/20  Loss: 0.2770\n",
      "Epoch: 8/20  Loss: 0.2585\n",
      "Epoch: 8/20  Loss: 0.2656\n",
      "Epoch: 9/20  Loss: 0.1695\n",
      "Epoch: 9/20  Loss: 0.2126\n",
      "Epoch: 9/20  Loss: 0.1988\n",
      "Epoch: 9/20  Loss: 0.2172\n",
      "Epoch: 9/20  Loss: 0.2082\n",
      "Epoch: 9/20  Loss: 0.2381\n",
      "Epoch: 9/20  Loss: 0.2093\n",
      "Epoch: 9/20  Loss: 0.2551\n",
      "Epoch: 9/20  Loss: 0.2125\n",
      "Epoch: 9/20  Loss: 0.2381\n",
      "Epoch: 10/20  Loss: 0.1824\n",
      "Epoch: 10/20  Loss: 0.1842\n",
      "Epoch: 10/20  Loss: 0.2010\n",
      "Epoch: 10/20  Loss: 0.1964\n",
      "Epoch: 10/20  Loss: 0.1640\n",
      "Epoch: 10/20  Loss: 0.2128\n",
      "Epoch: 10/20  Loss: 0.2024\n",
      "Epoch: 10/20  Loss: 0.1608\n",
      "Epoch: 10/20  Loss: 0.2018\n",
      "Epoch: 10/20  Loss: 0.2245\n",
      "Epoch: 11/20  Loss: 0.1606\n",
      "Epoch: 11/20  Loss: 0.1712\n",
      "Epoch: 11/20  Loss: 0.1604\n",
      "Epoch: 11/20  Loss: 0.1661\n",
      "Epoch: 11/20  Loss: 0.1799\n",
      "Epoch: 11/20  Loss: 0.1880\n",
      "Epoch: 11/20  Loss: 0.1879\n",
      "Epoch: 11/20  Loss: 0.1917\n",
      "Epoch: 11/20  Loss: 0.2438\n",
      "Epoch: 11/20  Loss: 0.1736\n",
      "Epoch: 12/20  Loss: 0.1383\n",
      "Epoch: 12/20  Loss: 0.1483\n",
      "Epoch: 12/20  Loss: 0.1488\n",
      "Epoch: 12/20  Loss: 0.1673\n",
      "Epoch: 12/20  Loss: 0.1674\n",
      "Epoch: 12/20  Loss: 0.1950\n",
      "Epoch: 12/20  Loss: 0.1429\n",
      "Epoch: 12/20  Loss: 0.1586\n",
      "Epoch: 12/20  Loss: 0.1904\n",
      "Epoch: 12/20  Loss: 0.1991\n",
      "Epoch: 13/20  Loss: 0.1391\n",
      "Epoch: 13/20  Loss: 0.1576\n",
      "Epoch: 13/20  Loss: 0.1194\n",
      "Epoch: 13/20  Loss: 0.1441\n",
      "Epoch: 13/20  Loss: 0.1306\n",
      "Epoch: 13/20  Loss: 0.1568\n",
      "Epoch: 13/20  Loss: 0.1441\n",
      "Epoch: 13/20  Loss: 0.1674\n",
      "Epoch: 13/20  Loss: 0.1580\n",
      "Epoch: 13/20  Loss: 0.1755\n",
      "Epoch: 14/20  Loss: 0.1335\n",
      "Epoch: 14/20  Loss: 0.1197\n",
      "Epoch: 14/20  Loss: 0.1450\n",
      "Epoch: 14/20  Loss: 0.1414\n",
      "Epoch: 14/20  Loss: 0.1076\n",
      "Epoch: 14/20  Loss: 0.1581\n",
      "Epoch: 14/20  Loss: 0.1396\n",
      "Epoch: 14/20  Loss: 0.1426\n",
      "Epoch: 14/20  Loss: 0.1375\n",
      "Epoch: 14/20  Loss: 0.1374\n",
      "Epoch: 15/20  Loss: 0.1003\n",
      "Epoch: 15/20  Loss: 0.1125\n",
      "Epoch: 15/20  Loss: 0.1178\n",
      "Epoch: 15/20  Loss: 0.1079\n",
      "Epoch: 15/20  Loss: 0.1377\n",
      "Epoch: 15/20  Loss: 0.0902\n",
      "Epoch: 15/20  Loss: 0.1399\n",
      "Epoch: 15/20  Loss: 0.1575\n",
      "Epoch: 15/20  Loss: 0.1617\n",
      "Epoch: 15/20  Loss: 0.1320\n",
      "Epoch: 16/20  Loss: 0.1099\n",
      "Epoch: 16/20  Loss: 0.1012\n",
      "Epoch: 16/20  Loss: 0.1033\n",
      "Epoch: 16/20  Loss: 0.1293\n",
      "Epoch: 16/20  Loss: 0.1105\n",
      "Epoch: 16/20  Loss: 0.1202\n",
      "Epoch: 16/20  Loss: 0.1062\n",
      "Epoch: 16/20  Loss: 0.1253\n",
      "Epoch: 16/20  Loss: 0.1105\n",
      "Epoch: 16/20  Loss: 0.1285\n",
      "Epoch: 17/20  Loss: 0.0987\n",
      "Epoch: 17/20  Loss: 0.1057\n",
      "Epoch: 17/20  Loss: 0.0839\n",
      "Epoch: 17/20  Loss: 0.1120\n",
      "Epoch: 17/20  Loss: 0.1121\n",
      "Epoch: 17/20  Loss: 0.1184\n",
      "Epoch: 17/20  Loss: 0.1195\n",
      "Epoch: 17/20  Loss: 0.1124\n",
      "Epoch: 17/20  Loss: 0.1389\n",
      "Epoch: 17/20  Loss: 0.1042\n",
      "Epoch: 18/20  Loss: 0.0790\n",
      "Epoch: 18/20  Loss: 0.1270\n",
      "Epoch: 18/20  Loss: 0.1123\n",
      "Epoch: 18/20  Loss: 0.0881\n",
      "Epoch: 18/20  Loss: 0.1049\n",
      "Epoch: 18/20  Loss: 0.0955\n",
      "Epoch: 18/20  Loss: 0.0969\n",
      "Epoch: 18/20  Loss: 0.1077\n",
      "Epoch: 18/20  Loss: 0.1152\n",
      "Epoch: 18/20  Loss: 0.1173\n",
      "Epoch: 19/20  Loss: 0.0775\n",
      "Epoch: 19/20  Loss: 0.0872\n",
      "Epoch: 19/20  Loss: 0.0879\n",
      "Epoch: 19/20  Loss: 0.0841\n",
      "Epoch: 19/20  Loss: 0.1010\n",
      "Epoch: 19/20  Loss: 0.1062\n",
      "Epoch: 19/20  Loss: 0.1060\n",
      "Epoch: 19/20  Loss: 0.1082\n",
      "Epoch: 19/20  Loss: 0.1028\n",
      "Epoch: 19/20  Loss: 0.1127\n",
      "Epoch: 20/20  Loss: 0.0806\n",
      "Epoch: 20/20  Loss: 0.0735\n",
      "Epoch: 20/20  Loss: 0.1001\n",
      "Epoch: 20/20  Loss: 0.0877\n",
      "Epoch: 20/20  Loss: 0.0753\n",
      "Epoch: 20/20  Loss: 0.0872\n",
      "Epoch: 20/20  Loss: 0.0960\n",
      "Epoch: 20/20  Loss: 0.0957\n",
      "Epoch: 20/20  Loss: 0.1299\n",
      "Epoch: 20/20  Loss: 0.1294\n",
      "Accuracy score for train set is 0.9850476190476191\n",
      "Accuracy score for test set is 0.8757777777777778\n",
      "Fitting model with epochs = 30, learning rate = 0.003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/30  Loss: 1.0894\n",
      "Epoch: 1/30  Loss: 0.7886\n",
      "Epoch: 1/30  Loss: 0.6891\n",
      "Epoch: 1/30  Loss: 0.6974\n",
      "Epoch: 1/30  Loss: 0.6305\n",
      "Epoch: 1/30  Loss: 0.6057\n",
      "Epoch: 1/30  Loss: 0.6276\n",
      "Epoch: 1/30  Loss: 0.5969\n",
      "Epoch: 1/30  Loss: 0.6074\n",
      "Epoch: 1/30  Loss: 0.5819\n",
      "Epoch: 2/30  Loss: 0.5057\n",
      "Epoch: 2/30  Loss: 0.5052\n",
      "Epoch: 2/30  Loss: 0.5348\n",
      "Epoch: 2/30  Loss: 0.5193\n",
      "Epoch: 2/30  Loss: 0.5096\n",
      "Epoch: 2/30  Loss: 0.5090\n",
      "Epoch: 2/30  Loss: 0.5069\n",
      "Epoch: 2/30  Loss: 0.5145\n",
      "Epoch: 2/30  Loss: 0.5036\n",
      "Epoch: 2/30  Loss: 0.4728\n",
      "Epoch: 3/30  Loss: 0.4302\n",
      "Epoch: 3/30  Loss: 0.4717\n",
      "Epoch: 3/30  Loss: 0.4393\n",
      "Epoch: 3/30  Loss: 0.4349\n",
      "Epoch: 3/30  Loss: 0.4268\n",
      "Epoch: 3/30  Loss: 0.4248\n",
      "Epoch: 3/30  Loss: 0.4247\n",
      "Epoch: 3/30  Loss: 0.4353\n",
      "Epoch: 3/30  Loss: 0.4166\n",
      "Epoch: 3/30  Loss: 0.4102\n",
      "Epoch: 4/30  Loss: 0.3501\n",
      "Epoch: 4/30  Loss: 0.3449\n",
      "Epoch: 4/30  Loss: 0.3712\n",
      "Epoch: 4/30  Loss: 0.3802\n",
      "Epoch: 4/30  Loss: 0.3666\n",
      "Epoch: 4/30  Loss: 0.3847\n",
      "Epoch: 4/30  Loss: 0.3765\n",
      "Epoch: 4/30  Loss: 0.3686\n",
      "Epoch: 4/30  Loss: 0.4319\n",
      "Epoch: 4/30  Loss: 0.3982\n",
      "Epoch: 5/30  Loss: 0.2971\n",
      "Epoch: 5/30  Loss: 0.3794\n",
      "Epoch: 5/30  Loss: 0.3422\n",
      "Epoch: 5/30  Loss: 0.3340\n",
      "Epoch: 5/30  Loss: 0.3185\n",
      "Epoch: 5/30  Loss: 0.3268\n",
      "Epoch: 5/30  Loss: 0.3626\n",
      "Epoch: 5/30  Loss: 0.3254\n",
      "Epoch: 5/30  Loss: 0.3114\n",
      "Epoch: 5/30  Loss: 0.3360\n",
      "Epoch: 6/30  Loss: 0.3071\n",
      "Epoch: 6/30  Loss: 0.2907\n",
      "Epoch: 6/30  Loss: 0.2612\n",
      "Epoch: 6/30  Loss: 0.3011\n",
      "Epoch: 6/30  Loss: 0.3192\n",
      "Epoch: 6/30  Loss: 0.3102\n",
      "Epoch: 6/30  Loss: 0.3244\n",
      "Epoch: 6/30  Loss: 0.3267\n",
      "Epoch: 6/30  Loss: 0.2948\n",
      "Epoch: 6/30  Loss: 0.3168\n",
      "Epoch: 7/30  Loss: 0.2264\n",
      "Epoch: 7/30  Loss: 0.2420\n",
      "Epoch: 7/30  Loss: 0.2837\n",
      "Epoch: 7/30  Loss: 0.2874\n",
      "Epoch: 7/30  Loss: 0.2652\n",
      "Epoch: 7/30  Loss: 0.2421\n",
      "Epoch: 7/30  Loss: 0.2640\n",
      "Epoch: 7/30  Loss: 0.2528\n",
      "Epoch: 7/30  Loss: 0.3017\n",
      "Epoch: 7/30  Loss: 0.3175\n",
      "Epoch: 8/30  Loss: 0.2290\n",
      "Epoch: 8/30  Loss: 0.2092\n",
      "Epoch: 8/30  Loss: 0.2372\n",
      "Epoch: 8/30  Loss: 0.2137\n",
      "Epoch: 8/30  Loss: 0.2217\n",
      "Epoch: 8/30  Loss: 0.2526\n",
      "Epoch: 8/30  Loss: 0.2693\n",
      "Epoch: 8/30  Loss: 0.2587\n",
      "Epoch: 8/30  Loss: 0.2635\n",
      "Epoch: 8/30  Loss: 0.2519\n",
      "Epoch: 9/30  Loss: 0.1954\n",
      "Epoch: 9/30  Loss: 0.1916\n",
      "Epoch: 9/30  Loss: 0.1909\n",
      "Epoch: 9/30  Loss: 0.1969\n",
      "Epoch: 9/30  Loss: 0.2259\n",
      "Epoch: 9/30  Loss: 0.2340\n",
      "Epoch: 9/30  Loss: 0.2001\n",
      "Epoch: 9/30  Loss: 0.2780\n",
      "Epoch: 9/30  Loss: 0.2480\n",
      "Epoch: 9/30  Loss: 0.2270\n",
      "Epoch: 10/30  Loss: 0.1747\n",
      "Epoch: 10/30  Loss: 0.1859\n",
      "Epoch: 10/30  Loss: 0.1669\n",
      "Epoch: 10/30  Loss: 0.1889\n",
      "Epoch: 10/30  Loss: 0.2165\n",
      "Epoch: 10/30  Loss: 0.1952\n",
      "Epoch: 10/30  Loss: 0.2017\n",
      "Epoch: 10/30  Loss: 0.2138\n",
      "Epoch: 10/30  Loss: 0.1908\n",
      "Epoch: 10/30  Loss: 0.2065\n",
      "Epoch: 11/30  Loss: 0.1310\n",
      "Epoch: 11/30  Loss: 0.1458\n",
      "Epoch: 11/30  Loss: 0.1687\n",
      "Epoch: 11/30  Loss: 0.1682\n",
      "Epoch: 11/30  Loss: 0.1887\n",
      "Epoch: 11/30  Loss: 0.1590\n",
      "Epoch: 11/30  Loss: 0.1926\n",
      "Epoch: 11/30  Loss: 0.1975\n",
      "Epoch: 11/30  Loss: 0.2018\n",
      "Epoch: 11/30  Loss: 0.2078\n",
      "Epoch: 12/30  Loss: 0.1449\n",
      "Epoch: 12/30  Loss: 0.1297\n",
      "Epoch: 12/30  Loss: 0.1325\n",
      "Epoch: 12/30  Loss: 0.1456\n",
      "Epoch: 12/30  Loss: 0.1846\n",
      "Epoch: 12/30  Loss: 0.1815\n",
      "Epoch: 12/30  Loss: 0.1620\n",
      "Epoch: 12/30  Loss: 0.1807\n",
      "Epoch: 12/30  Loss: 0.1653\n",
      "Epoch: 12/30  Loss: 0.1798\n",
      "Epoch: 13/30  Loss: 0.1222\n",
      "Epoch: 13/30  Loss: 0.1254\n",
      "Epoch: 13/30  Loss: 0.1437\n",
      "Epoch: 13/30  Loss: 0.1562\n",
      "Epoch: 13/30  Loss: 0.1219\n",
      "Epoch: 13/30  Loss: 0.1581\n",
      "Epoch: 13/30  Loss: 0.1296\n",
      "Epoch: 13/30  Loss: 0.1801\n",
      "Epoch: 13/30  Loss: 0.1370\n",
      "Epoch: 13/30  Loss: 0.1613\n",
      "Epoch: 14/30  Loss: 0.1166\n",
      "Epoch: 14/30  Loss: 0.1242\n",
      "Epoch: 14/30  Loss: 0.1291\n",
      "Epoch: 14/30  Loss: 0.1302\n",
      "Epoch: 14/30  Loss: 0.1587\n",
      "Epoch: 14/30  Loss: 0.1446\n",
      "Epoch: 14/30  Loss: 0.1419\n",
      "Epoch: 14/30  Loss: 0.1590\n",
      "Epoch: 14/30  Loss: 0.1252\n",
      "Epoch: 14/30  Loss: 0.1411\n",
      "Epoch: 15/30  Loss: 0.0982\n",
      "Epoch: 15/30  Loss: 0.0804\n",
      "Epoch: 15/30  Loss: 0.1073\n",
      "Epoch: 15/30  Loss: 0.1277\n",
      "Epoch: 15/30  Loss: 0.1262\n",
      "Epoch: 15/30  Loss: 0.1454\n",
      "Epoch: 15/30  Loss: 0.1308\n",
      "Epoch: 15/30  Loss: 0.1267\n",
      "Epoch: 15/30  Loss: 0.1672\n",
      "Epoch: 15/30  Loss: 0.1378\n",
      "Epoch: 16/30  Loss: 0.1035\n",
      "Epoch: 16/30  Loss: 0.0979\n",
      "Epoch: 16/30  Loss: 0.1086\n",
      "Epoch: 16/30  Loss: 0.1010\n",
      "Epoch: 16/30  Loss: 0.1244\n",
      "Epoch: 16/30  Loss: 0.1381\n",
      "Epoch: 16/30  Loss: 0.1398\n",
      "Epoch: 16/30  Loss: 0.1172\n",
      "Epoch: 16/30  Loss: 0.1046\n",
      "Epoch: 16/30  Loss: 0.1363\n",
      "Epoch: 17/30  Loss: 0.0942\n",
      "Epoch: 17/30  Loss: 0.1034\n",
      "Epoch: 17/30  Loss: 0.1144\n",
      "Epoch: 17/30  Loss: 0.0952\n",
      "Epoch: 17/30  Loss: 0.1063\n",
      "Epoch: 17/30  Loss: 0.1082\n",
      "Epoch: 17/30  Loss: 0.1059\n",
      "Epoch: 17/30  Loss: 0.1249\n",
      "Epoch: 17/30  Loss: 0.1153\n",
      "Epoch: 17/30  Loss: 0.1398\n",
      "Epoch: 18/30  Loss: 0.0866\n",
      "Epoch: 18/30  Loss: 0.0804\n",
      "Epoch: 18/30  Loss: 0.0869\n",
      "Epoch: 18/30  Loss: 0.0939\n",
      "Epoch: 18/30  Loss: 0.1065\n",
      "Epoch: 18/30  Loss: 0.0804\n",
      "Epoch: 18/30  Loss: 0.0960\n",
      "Epoch: 18/30  Loss: 0.1107\n",
      "Epoch: 18/30  Loss: 0.1323\n",
      "Epoch: 18/30  Loss: 0.1073\n",
      "Epoch: 19/30  Loss: 0.0858\n",
      "Epoch: 19/30  Loss: 0.0760\n",
      "Epoch: 19/30  Loss: 0.0992\n",
      "Epoch: 19/30  Loss: 0.1060\n",
      "Epoch: 19/30  Loss: 0.0816\n",
      "Epoch: 19/30  Loss: 0.0875\n",
      "Epoch: 19/30  Loss: 0.1024\n",
      "Epoch: 19/30  Loss: 0.0886\n",
      "Epoch: 19/30  Loss: 0.1164\n",
      "Epoch: 19/30  Loss: 0.1164\n",
      "Epoch: 20/30  Loss: 0.0754\n",
      "Epoch: 20/30  Loss: 0.0682\n",
      "Epoch: 20/30  Loss: 0.0764\n",
      "Epoch: 20/30  Loss: 0.0825\n",
      "Epoch: 20/30  Loss: 0.0947\n",
      "Epoch: 20/30  Loss: 0.1033\n",
      "Epoch: 20/30  Loss: 0.0954\n",
      "Epoch: 20/30  Loss: 0.0949\n",
      "Epoch: 20/30  Loss: 0.0941\n",
      "Epoch: 20/30  Loss: 0.0995\n",
      "Epoch: 21/30  Loss: 0.0518\n",
      "Epoch: 21/30  Loss: 0.0874\n",
      "Epoch: 21/30  Loss: 0.0778\n",
      "Epoch: 21/30  Loss: 0.0988\n",
      "Epoch: 21/30  Loss: 0.0749\n",
      "Epoch: 21/30  Loss: 0.0796\n",
      "Epoch: 21/30  Loss: 0.0753\n",
      "Epoch: 21/30  Loss: 0.1104\n",
      "Epoch: 21/30  Loss: 0.1132\n",
      "Epoch: 21/30  Loss: 0.0799\n",
      "Epoch: 22/30  Loss: 0.0606\n",
      "Epoch: 22/30  Loss: 0.0753\n",
      "Epoch: 22/30  Loss: 0.0786\n",
      "Epoch: 22/30  Loss: 0.0671\n",
      "Epoch: 22/30  Loss: 0.0586\n",
      "Epoch: 22/30  Loss: 0.0832\n",
      "Epoch: 22/30  Loss: 0.0809\n",
      "Epoch: 22/30  Loss: 0.0624\n",
      "Epoch: 22/30  Loss: 0.0816\n",
      "Epoch: 22/30  Loss: 0.0800\n",
      "Epoch: 23/30  Loss: 0.0536\n",
      "Epoch: 23/30  Loss: 0.0782\n",
      "Epoch: 23/30  Loss: 0.0842\n",
      "Epoch: 23/30  Loss: 0.0701\n",
      "Epoch: 23/30  Loss: 0.0795\n",
      "Epoch: 23/30  Loss: 0.0588\n",
      "Epoch: 23/30  Loss: 0.0619\n",
      "Epoch: 23/30  Loss: 0.0842\n",
      "Epoch: 23/30  Loss: 0.0855\n",
      "Epoch: 23/30  Loss: 0.0737\n",
      "Epoch: 24/30  Loss: 0.0789\n",
      "Epoch: 24/30  Loss: 0.0718\n",
      "Epoch: 24/30  Loss: 0.0731\n",
      "Epoch: 24/30  Loss: 0.0817\n",
      "Epoch: 24/30  Loss: 0.0670\n",
      "Epoch: 24/30  Loss: 0.0536\n",
      "Epoch: 24/30  Loss: 0.0874\n",
      "Epoch: 24/30  Loss: 0.0668\n",
      "Epoch: 24/30  Loss: 0.0809\n",
      "Epoch: 24/30  Loss: 0.0894\n",
      "Epoch: 25/30  Loss: 0.0844\n",
      "Epoch: 25/30  Loss: 0.0561\n",
      "Epoch: 25/30  Loss: 0.0583\n",
      "Epoch: 25/30  Loss: 0.0833\n",
      "Epoch: 25/30  Loss: 0.0718\n",
      "Epoch: 25/30  Loss: 0.0786\n",
      "Epoch: 25/30  Loss: 0.0707\n",
      "Epoch: 25/30  Loss: 0.0591\n",
      "Epoch: 25/30  Loss: 0.0647\n",
      "Epoch: 25/30  Loss: 0.1012\n",
      "Epoch: 26/30  Loss: 0.0701\n",
      "Epoch: 26/30  Loss: 0.0603\n",
      "Epoch: 26/30  Loss: 0.0479\n",
      "Epoch: 26/30  Loss: 0.0608\n",
      "Epoch: 26/30  Loss: 0.0759\n",
      "Epoch: 26/30  Loss: 0.0673\n",
      "Epoch: 26/30  Loss: 0.0766\n",
      "Epoch: 26/30  Loss: 0.0820\n",
      "Epoch: 26/30  Loss: 0.0562\n",
      "Epoch: 26/30  Loss: 0.0587\n",
      "Epoch: 27/30  Loss: 0.0465\n",
      "Epoch: 27/30  Loss: 0.0422\n",
      "Epoch: 27/30  Loss: 0.0638\n",
      "Epoch: 27/30  Loss: 0.0594\n",
      "Epoch: 27/30  Loss: 0.0655\n",
      "Epoch: 27/30  Loss: 0.0613\n",
      "Epoch: 27/30  Loss: 0.0600\n",
      "Epoch: 27/30  Loss: 0.0566\n",
      "Epoch: 27/30  Loss: 0.0700\n",
      "Epoch: 27/30  Loss: 0.0716\n",
      "Epoch: 28/30  Loss: 0.0514\n",
      "Epoch: 28/30  Loss: 0.0595\n",
      "Epoch: 28/30  Loss: 0.0522\n",
      "Epoch: 28/30  Loss: 0.0533\n",
      "Epoch: 28/30  Loss: 0.0524\n",
      "Epoch: 28/30  Loss: 0.0703\n",
      "Epoch: 28/30  Loss: 0.0567\n",
      "Epoch: 28/30  Loss: 0.0651\n",
      "Epoch: 28/30  Loss: 0.0799\n",
      "Epoch: 28/30  Loss: 0.0732\n",
      "Epoch: 29/30  Loss: 0.0497\n",
      "Epoch: 29/30  Loss: 0.0484\n",
      "Epoch: 29/30  Loss: 0.0498\n",
      "Epoch: 29/30  Loss: 0.0791\n",
      "Epoch: 29/30  Loss: 0.0477\n",
      "Epoch: 29/30  Loss: 0.0700\n",
      "Epoch: 29/30  Loss: 0.0684\n",
      "Epoch: 29/30  Loss: 0.0913\n",
      "Epoch: 29/30  Loss: 0.0789\n",
      "Epoch: 29/30  Loss: 0.0686\n",
      "Epoch: 30/30  Loss: 0.0471\n",
      "Epoch: 30/30  Loss: 0.0439\n",
      "Epoch: 30/30  Loss: 0.0459\n",
      "Epoch: 30/30  Loss: 0.0409\n",
      "Epoch: 30/30  Loss: 0.0423\n",
      "Epoch: 30/30  Loss: 0.0522\n",
      "Epoch: 30/30  Loss: 0.0490\n",
      "Epoch: 30/30  Loss: 0.0520\n",
      "Epoch: 30/30  Loss: 0.0591\n",
      "Epoch: 30/30  Loss: 0.0557\n",
      "Accuracy score for train set is 0.9948095238095238\n",
      "Accuracy score for test set is 0.877\n",
      "Fitting model with epochs = 40, learning rate = 0.003\n",
      "Epoch: 1/40  Loss: 1.0618\n",
      "Epoch: 1/40  Loss: 0.7969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/40  Loss: 0.7394\n",
      "Epoch: 1/40  Loss: 0.6982\n",
      "Epoch: 1/40  Loss: 0.6296\n",
      "Epoch: 1/40  Loss: 0.6219\n",
      "Epoch: 1/40  Loss: 0.6322\n",
      "Epoch: 1/40  Loss: 0.5634\n",
      "Epoch: 1/40  Loss: 0.5575\n",
      "Epoch: 1/40  Loss: 0.5732\n",
      "Epoch: 2/40  Loss: 0.4893\n",
      "Epoch: 2/40  Loss: 0.5106\n",
      "Epoch: 2/40  Loss: 0.5166\n",
      "Epoch: 2/40  Loss: 0.5111\n",
      "Epoch: 2/40  Loss: 0.4716\n",
      "Epoch: 2/40  Loss: 0.5482\n",
      "Epoch: 2/40  Loss: 0.4686\n",
      "Epoch: 2/40  Loss: 0.5231\n",
      "Epoch: 2/40  Loss: 0.4928\n",
      "Epoch: 2/40  Loss: 0.4820\n",
      "Epoch: 3/40  Loss: 0.4222\n",
      "Epoch: 3/40  Loss: 0.3925\n",
      "Epoch: 3/40  Loss: 0.4406\n",
      "Epoch: 3/40  Loss: 0.4114\n",
      "Epoch: 3/40  Loss: 0.4067\n",
      "Epoch: 3/40  Loss: 0.4588\n",
      "Epoch: 3/40  Loss: 0.3757\n",
      "Epoch: 3/40  Loss: 0.4284\n",
      "Epoch: 3/40  Loss: 0.4428\n",
      "Epoch: 3/40  Loss: 0.4623\n",
      "Epoch: 4/40  Loss: 0.3453\n",
      "Epoch: 4/40  Loss: 0.3329\n",
      "Epoch: 4/40  Loss: 0.3835\n",
      "Epoch: 4/40  Loss: 0.3739\n",
      "Epoch: 4/40  Loss: 0.3700\n",
      "Epoch: 4/40  Loss: 0.3805\n",
      "Epoch: 4/40  Loss: 0.3832\n",
      "Epoch: 4/40  Loss: 0.4313\n",
      "Epoch: 4/40  Loss: 0.3804\n",
      "Epoch: 4/40  Loss: 0.3908\n",
      "Epoch: 5/40  Loss: 0.3481\n",
      "Epoch: 5/40  Loss: 0.2815\n",
      "Epoch: 5/40  Loss: 0.3304\n",
      "Epoch: 5/40  Loss: 0.3381\n",
      "Epoch: 5/40  Loss: 0.3331\n",
      "Epoch: 5/40  Loss: 0.3297\n",
      "Epoch: 5/40  Loss: 0.3759\n",
      "Epoch: 5/40  Loss: 0.3180\n",
      "Epoch: 5/40  Loss: 0.3278\n",
      "Epoch: 5/40  Loss: 0.3433\n",
      "Epoch: 6/40  Loss: 0.2829\n",
      "Epoch: 6/40  Loss: 0.2763\n",
      "Epoch: 6/40  Loss: 0.3141\n",
      "Epoch: 6/40  Loss: 0.2986\n",
      "Epoch: 6/40  Loss: 0.2697\n",
      "Epoch: 6/40  Loss: 0.2803\n",
      "Epoch: 6/40  Loss: 0.3232\n",
      "Epoch: 6/40  Loss: 0.3078\n",
      "Epoch: 6/40  Loss: 0.3507\n",
      "Epoch: 6/40  Loss: 0.2966\n",
      "Epoch: 7/40  Loss: 0.2233\n",
      "Epoch: 7/40  Loss: 0.2299\n",
      "Epoch: 7/40  Loss: 0.2600\n",
      "Epoch: 7/40  Loss: 0.2654\n",
      "Epoch: 7/40  Loss: 0.2931\n",
      "Epoch: 7/40  Loss: 0.2423\n",
      "Epoch: 7/40  Loss: 0.2944\n",
      "Epoch: 7/40  Loss: 0.2981\n",
      "Epoch: 7/40  Loss: 0.2832\n",
      "Epoch: 7/40  Loss: 0.2904\n",
      "Epoch: 8/40  Loss: 0.2151\n",
      "Epoch: 8/40  Loss: 0.2011\n",
      "Epoch: 8/40  Loss: 0.2554\n",
      "Epoch: 8/40  Loss: 0.2678\n",
      "Epoch: 8/40  Loss: 0.2411\n",
      "Epoch: 8/40  Loss: 0.2805\n",
      "Epoch: 8/40  Loss: 0.2198\n",
      "Epoch: 8/40  Loss: 0.2598\n",
      "Epoch: 8/40  Loss: 0.2426\n",
      "Epoch: 8/40  Loss: 0.2697\n",
      "Epoch: 9/40  Loss: 0.1942\n",
      "Epoch: 9/40  Loss: 0.1930\n",
      "Epoch: 9/40  Loss: 0.2186\n",
      "Epoch: 9/40  Loss: 0.2428\n",
      "Epoch: 9/40  Loss: 0.1951\n",
      "Epoch: 9/40  Loss: 0.2205\n",
      "Epoch: 9/40  Loss: 0.2394\n",
      "Epoch: 9/40  Loss: 0.2172\n",
      "Epoch: 9/40  Loss: 0.2219\n",
      "Epoch: 9/40  Loss: 0.2620\n",
      "Epoch: 10/40  Loss: 0.1915\n",
      "Epoch: 10/40  Loss: 0.2011\n",
      "Epoch: 10/40  Loss: 0.1729\n",
      "Epoch: 10/40  Loss: 0.1723\n",
      "Epoch: 10/40  Loss: 0.2152\n",
      "Epoch: 10/40  Loss: 0.2090\n",
      "Epoch: 10/40  Loss: 0.1860\n",
      "Epoch: 10/40  Loss: 0.2028\n",
      "Epoch: 10/40  Loss: 0.2139\n",
      "Epoch: 10/40  Loss: 0.2141\n",
      "Epoch: 11/40  Loss: 0.1648\n",
      "Epoch: 11/40  Loss: 0.1493\n",
      "Epoch: 11/40  Loss: 0.1756\n",
      "Epoch: 11/40  Loss: 0.1627\n",
      "Epoch: 11/40  Loss: 0.2111\n",
      "Epoch: 11/40  Loss: 0.1628\n",
      "Epoch: 11/40  Loss: 0.1593\n",
      "Epoch: 11/40  Loss: 0.1869\n",
      "Epoch: 11/40  Loss: 0.2147\n",
      "Epoch: 11/40  Loss: 0.1812\n",
      "Epoch: 12/40  Loss: 0.1372\n",
      "Epoch: 12/40  Loss: 0.1309\n",
      "Epoch: 12/40  Loss: 0.1513\n",
      "Epoch: 12/40  Loss: 0.1522\n",
      "Epoch: 12/40  Loss: 0.1999\n",
      "Epoch: 12/40  Loss: 0.1747\n",
      "Epoch: 12/40  Loss: 0.1969\n",
      "Epoch: 12/40  Loss: 0.1970\n",
      "Epoch: 12/40  Loss: 0.1731\n",
      "Epoch: 12/40  Loss: 0.1723\n",
      "Epoch: 13/40  Loss: 0.1591\n",
      "Epoch: 13/40  Loss: 0.0958\n",
      "Epoch: 13/40  Loss: 0.1511\n",
      "Epoch: 13/40  Loss: 0.1519\n",
      "Epoch: 13/40  Loss: 0.1562\n",
      "Epoch: 13/40  Loss: 0.1459\n",
      "Epoch: 13/40  Loss: 0.1761\n",
      "Epoch: 13/40  Loss: 0.1665\n",
      "Epoch: 13/40  Loss: 0.1765\n",
      "Epoch: 13/40  Loss: 0.1510\n",
      "Epoch: 14/40  Loss: 0.1026\n",
      "Epoch: 14/40  Loss: 0.1214\n",
      "Epoch: 14/40  Loss: 0.1272\n",
      "Epoch: 14/40  Loss: 0.1481\n",
      "Epoch: 14/40  Loss: 0.1290\n",
      "Epoch: 14/40  Loss: 0.1496\n",
      "Epoch: 14/40  Loss: 0.1596\n",
      "Epoch: 14/40  Loss: 0.1537\n",
      "Epoch: 14/40  Loss: 0.1617\n",
      "Epoch: 14/40  Loss: 0.1598\n",
      "Epoch: 15/40  Loss: 0.1141\n",
      "Epoch: 15/40  Loss: 0.1190\n",
      "Epoch: 15/40  Loss: 0.1091\n",
      "Epoch: 15/40  Loss: 0.1148\n",
      "Epoch: 15/40  Loss: 0.1235\n",
      "Epoch: 15/40  Loss: 0.1213\n",
      "Epoch: 15/40  Loss: 0.1350\n",
      "Epoch: 15/40  Loss: 0.1697\n",
      "Epoch: 15/40  Loss: 0.1534\n",
      "Epoch: 15/40  Loss: 0.1272\n",
      "Epoch: 16/40  Loss: 0.1044\n",
      "Epoch: 16/40  Loss: 0.1081\n",
      "Epoch: 16/40  Loss: 0.1025\n",
      "Epoch: 16/40  Loss: 0.0892\n",
      "Epoch: 16/40  Loss: 0.1145\n",
      "Epoch: 16/40  Loss: 0.1508\n",
      "Epoch: 16/40  Loss: 0.1423\n",
      "Epoch: 16/40  Loss: 0.1446\n",
      "Epoch: 16/40  Loss: 0.1185\n",
      "Epoch: 16/40  Loss: 0.1520\n",
      "Epoch: 17/40  Loss: 0.0857\n",
      "Epoch: 17/40  Loss: 0.1120\n",
      "Epoch: 17/40  Loss: 0.1203\n",
      "Epoch: 17/40  Loss: 0.1031\n",
      "Epoch: 17/40  Loss: 0.1171\n",
      "Epoch: 17/40  Loss: 0.1199\n",
      "Epoch: 17/40  Loss: 0.1140\n",
      "Epoch: 17/40  Loss: 0.1293\n",
      "Epoch: 17/40  Loss: 0.1462\n",
      "Epoch: 17/40  Loss: 0.1026\n",
      "Epoch: 18/40  Loss: 0.0980\n",
      "Epoch: 18/40  Loss: 0.0978\n",
      "Epoch: 18/40  Loss: 0.0783\n",
      "Epoch: 18/40  Loss: 0.1094\n",
      "Epoch: 18/40  Loss: 0.1071\n",
      "Epoch: 18/40  Loss: 0.0866\n",
      "Epoch: 18/40  Loss: 0.1449\n",
      "Epoch: 18/40  Loss: 0.1208\n",
      "Epoch: 18/40  Loss: 0.1126\n",
      "Epoch: 18/40  Loss: 0.1200\n",
      "Epoch: 19/40  Loss: 0.0801\n",
      "Epoch: 19/40  Loss: 0.0643\n",
      "Epoch: 19/40  Loss: 0.0777\n",
      "Epoch: 19/40  Loss: 0.1025\n",
      "Epoch: 19/40  Loss: 0.1089\n",
      "Epoch: 19/40  Loss: 0.0805\n",
      "Epoch: 19/40  Loss: 0.1018\n",
      "Epoch: 19/40  Loss: 0.1165\n",
      "Epoch: 19/40  Loss: 0.1198\n",
      "Epoch: 19/40  Loss: 0.1040\n",
      "Epoch: 20/40  Loss: 0.0856\n",
      "Epoch: 20/40  Loss: 0.0687\n",
      "Epoch: 20/40  Loss: 0.0765\n",
      "Epoch: 20/40  Loss: 0.0886\n",
      "Epoch: 20/40  Loss: 0.1049\n",
      "Epoch: 20/40  Loss: 0.1018\n",
      "Epoch: 20/40  Loss: 0.0820\n",
      "Epoch: 20/40  Loss: 0.0915\n",
      "Epoch: 20/40  Loss: 0.1041\n",
      "Epoch: 20/40  Loss: 0.1286\n",
      "Epoch: 21/40  Loss: 0.0805\n",
      "Epoch: 21/40  Loss: 0.1001\n",
      "Epoch: 21/40  Loss: 0.0845\n",
      "Epoch: 21/40  Loss: 0.0913\n",
      "Epoch: 21/40  Loss: 0.0931\n",
      "Epoch: 21/40  Loss: 0.0979\n",
      "Epoch: 21/40  Loss: 0.1014\n",
      "Epoch: 21/40  Loss: 0.0962\n",
      "Epoch: 21/40  Loss: 0.0972\n",
      "Epoch: 21/40  Loss: 0.1019\n",
      "Epoch: 22/40  Loss: 0.0779\n",
      "Epoch: 22/40  Loss: 0.0684\n",
      "Epoch: 22/40  Loss: 0.0638\n",
      "Epoch: 22/40  Loss: 0.0692\n",
      "Epoch: 22/40  Loss: 0.0721\n",
      "Epoch: 22/40  Loss: 0.0903\n",
      "Epoch: 22/40  Loss: 0.0765\n",
      "Epoch: 22/40  Loss: 0.0743\n",
      "Epoch: 22/40  Loss: 0.1280\n",
      "Epoch: 22/40  Loss: 0.0811\n",
      "Epoch: 23/40  Loss: 0.0622\n",
      "Epoch: 23/40  Loss: 0.0843\n",
      "Epoch: 23/40  Loss: 0.0813\n",
      "Epoch: 23/40  Loss: 0.0705\n",
      "Epoch: 23/40  Loss: 0.0650\n",
      "Epoch: 23/40  Loss: 0.0841\n",
      "Epoch: 23/40  Loss: 0.0867\n",
      "Epoch: 23/40  Loss: 0.0787\n",
      "Epoch: 23/40  Loss: 0.0809\n",
      "Epoch: 23/40  Loss: 0.0867\n",
      "Epoch: 24/40  Loss: 0.0638\n",
      "Epoch: 24/40  Loss: 0.0819\n",
      "Epoch: 24/40  Loss: 0.0913\n",
      "Epoch: 24/40  Loss: 0.0750\n",
      "Epoch: 24/40  Loss: 0.0650\n",
      "Epoch: 24/40  Loss: 0.0882\n",
      "Epoch: 24/40  Loss: 0.0845\n",
      "Epoch: 24/40  Loss: 0.0680\n",
      "Epoch: 24/40  Loss: 0.0834\n",
      "Epoch: 24/40  Loss: 0.1162\n",
      "Epoch: 25/40  Loss: 0.0532\n",
      "Epoch: 25/40  Loss: 0.0645\n",
      "Epoch: 25/40  Loss: 0.0510\n",
      "Epoch: 25/40  Loss: 0.0758\n",
      "Epoch: 25/40  Loss: 0.0657\n",
      "Epoch: 25/40  Loss: 0.0713\n",
      "Epoch: 25/40  Loss: 0.0590\n",
      "Epoch: 25/40  Loss: 0.0714\n",
      "Epoch: 25/40  Loss: 0.0944\n",
      "Epoch: 25/40  Loss: 0.0836\n",
      "Epoch: 26/40  Loss: 0.0612\n",
      "Epoch: 26/40  Loss: 0.0468\n",
      "Epoch: 26/40  Loss: 0.0556\n",
      "Epoch: 26/40  Loss: 0.0477\n",
      "Epoch: 26/40  Loss: 0.0563\n",
      "Epoch: 26/40  Loss: 0.0746\n",
      "Epoch: 26/40  Loss: 0.0773\n",
      "Epoch: 26/40  Loss: 0.0877\n",
      "Epoch: 26/40  Loss: 0.0726\n",
      "Epoch: 26/40  Loss: 0.0739\n",
      "Epoch: 27/40  Loss: 0.0742\n",
      "Epoch: 27/40  Loss: 0.0688\n",
      "Epoch: 27/40  Loss: 0.0536\n",
      "Epoch: 27/40  Loss: 0.0794\n",
      "Epoch: 27/40  Loss: 0.0638\n",
      "Epoch: 27/40  Loss: 0.0612\n",
      "Epoch: 27/40  Loss: 0.0748\n",
      "Epoch: 27/40  Loss: 0.0624\n",
      "Epoch: 27/40  Loss: 0.0683\n",
      "Epoch: 27/40  Loss: 0.0869\n",
      "Epoch: 28/40  Loss: 0.0595\n",
      "Epoch: 28/40  Loss: 0.0743\n",
      "Epoch: 28/40  Loss: 0.0545\n",
      "Epoch: 28/40  Loss: 0.0591\n",
      "Epoch: 28/40  Loss: 0.0563\n",
      "Epoch: 28/40  Loss: 0.0679\n",
      "Epoch: 28/40  Loss: 0.0691\n",
      "Epoch: 28/40  Loss: 0.0612\n",
      "Epoch: 28/40  Loss: 0.0872\n",
      "Epoch: 28/40  Loss: 0.0766\n",
      "Epoch: 29/40  Loss: 0.0661\n",
      "Epoch: 29/40  Loss: 0.0528\n",
      "Epoch: 29/40  Loss: 0.0579\n",
      "Epoch: 29/40  Loss: 0.0559\n",
      "Epoch: 29/40  Loss: 0.0709\n",
      "Epoch: 29/40  Loss: 0.0709\n",
      "Epoch: 29/40  Loss: 0.0768\n",
      "Epoch: 29/40  Loss: 0.0690\n",
      "Epoch: 29/40  Loss: 0.0639\n",
      "Epoch: 29/40  Loss: 0.0723\n",
      "Epoch: 30/40  Loss: 0.0427\n",
      "Epoch: 30/40  Loss: 0.0477\n",
      "Epoch: 30/40  Loss: 0.0391\n",
      "Epoch: 30/40  Loss: 0.0600\n",
      "Epoch: 30/40  Loss: 0.0577\n",
      "Epoch: 30/40  Loss: 0.0656\n",
      "Epoch: 30/40  Loss: 0.0753\n",
      "Epoch: 30/40  Loss: 0.0681\n",
      "Epoch: 30/40  Loss: 0.0657\n",
      "Epoch: 30/40  Loss: 0.0823\n",
      "Epoch: 31/40  Loss: 0.0442\n",
      "Epoch: 31/40  Loss: 0.0388\n",
      "Epoch: 31/40  Loss: 0.0672\n",
      "Epoch: 31/40  Loss: 0.0356\n",
      "Epoch: 31/40  Loss: 0.0638\n",
      "Epoch: 31/40  Loss: 0.0554\n",
      "Epoch: 31/40  Loss: 0.0565\n",
      "Epoch: 31/40  Loss: 0.0628\n",
      "Epoch: 31/40  Loss: 0.0531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31/40  Loss: 0.0513\n",
      "Epoch: 32/40  Loss: 0.0400\n",
      "Epoch: 32/40  Loss: 0.0546\n",
      "Epoch: 32/40  Loss: 0.0495\n",
      "Epoch: 32/40  Loss: 0.0460\n",
      "Epoch: 32/40  Loss: 0.0343\n",
      "Epoch: 32/40  Loss: 0.0526\n",
      "Epoch: 32/40  Loss: 0.0587\n",
      "Epoch: 32/40  Loss: 0.0848\n",
      "Epoch: 32/40  Loss: 0.0617\n",
      "Epoch: 32/40  Loss: 0.0609\n",
      "Epoch: 33/40  Loss: 0.0386\n",
      "Epoch: 33/40  Loss: 0.0430\n",
      "Epoch: 33/40  Loss: 0.0433\n",
      "Epoch: 33/40  Loss: 0.0546\n",
      "Epoch: 33/40  Loss: 0.0453\n",
      "Epoch: 33/40  Loss: 0.0541\n",
      "Epoch: 33/40  Loss: 0.0451\n",
      "Epoch: 33/40  Loss: 0.0556\n",
      "Epoch: 33/40  Loss: 0.0529\n",
      "Epoch: 33/40  Loss: 0.0483\n",
      "Epoch: 34/40  Loss: 0.0426\n",
      "Epoch: 34/40  Loss: 0.0511\n",
      "Epoch: 34/40  Loss: 0.0377\n",
      "Epoch: 34/40  Loss: 0.0417\n",
      "Epoch: 34/40  Loss: 0.0582\n",
      "Epoch: 34/40  Loss: 0.0443\n",
      "Epoch: 34/40  Loss: 0.0516\n",
      "Epoch: 34/40  Loss: 0.0382\n",
      "Epoch: 34/40  Loss: 0.0592\n",
      "Epoch: 34/40  Loss: 0.0742\n",
      "Epoch: 35/40  Loss: 0.0661\n",
      "Epoch: 35/40  Loss: 0.0418\n",
      "Epoch: 35/40  Loss: 0.0465\n",
      "Epoch: 35/40  Loss: 0.0543\n",
      "Epoch: 35/40  Loss: 0.0445\n",
      "Epoch: 35/40  Loss: 0.0548\n",
      "Epoch: 35/40  Loss: 0.0520\n",
      "Epoch: 35/40  Loss: 0.0423\n",
      "Epoch: 35/40  Loss: 0.0558\n",
      "Epoch: 35/40  Loss: 0.0595\n",
      "Epoch: 36/40  Loss: 0.0403\n",
      "Epoch: 36/40  Loss: 0.0335\n",
      "Epoch: 36/40  Loss: 0.0346\n",
      "Epoch: 36/40  Loss: 0.0415\n",
      "Epoch: 36/40  Loss: 0.0384\n",
      "Epoch: 36/40  Loss: 0.0510\n",
      "Epoch: 36/40  Loss: 0.0573\n",
      "Epoch: 36/40  Loss: 0.0568\n",
      "Epoch: 36/40  Loss: 0.0527\n",
      "Epoch: 36/40  Loss: 0.0839\n",
      "Epoch: 37/40  Loss: 0.0529\n",
      "Epoch: 37/40  Loss: 0.0420\n",
      "Epoch: 37/40  Loss: 0.0460\n",
      "Epoch: 37/40  Loss: 0.0415\n",
      "Epoch: 37/40  Loss: 0.0486\n",
      "Epoch: 37/40  Loss: 0.0443\n",
      "Epoch: 37/40  Loss: 0.0454\n",
      "Epoch: 37/40  Loss: 0.0615\n",
      "Epoch: 37/40  Loss: 0.0495\n",
      "Epoch: 37/40  Loss: 0.0539\n",
      "Epoch: 38/40  Loss: 0.0359\n",
      "Epoch: 38/40  Loss: 0.0483\n",
      "Epoch: 38/40  Loss: 0.0535\n",
      "Epoch: 38/40  Loss: 0.0501\n",
      "Epoch: 38/40  Loss: 0.0394\n",
      "Epoch: 38/40  Loss: 0.0467\n",
      "Epoch: 38/40  Loss: 0.0625\n",
      "Epoch: 38/40  Loss: 0.0487\n",
      "Epoch: 38/40  Loss: 0.0436\n",
      "Epoch: 38/40  Loss: 0.0462\n",
      "Epoch: 39/40  Loss: 0.0238\n",
      "Epoch: 39/40  Loss: 0.0324\n",
      "Epoch: 39/40  Loss: 0.0352\n",
      "Epoch: 39/40  Loss: 0.0493\n",
      "Epoch: 39/40  Loss: 0.0388\n",
      "Epoch: 39/40  Loss: 0.0592\n",
      "Epoch: 39/40  Loss: 0.0448\n",
      "Epoch: 39/40  Loss: 0.0480\n",
      "Epoch: 39/40  Loss: 0.0480\n",
      "Epoch: 39/40  Loss: 0.0505\n",
      "Epoch: 40/40  Loss: 0.0307\n",
      "Epoch: 40/40  Loss: 0.0288\n",
      "Epoch: 40/40  Loss: 0.0364\n",
      "Epoch: 40/40  Loss: 0.0454\n",
      "Epoch: 40/40  Loss: 0.0321\n",
      "Epoch: 40/40  Loss: 0.0512\n",
      "Epoch: 40/40  Loss: 0.0495\n",
      "Epoch: 40/40  Loss: 0.0548\n",
      "Epoch: 40/40  Loss: 0.0493\n",
      "Epoch: 40/40  Loss: 0.0572\n",
      "Accuracy score for train set is 0.9943809523809524\n",
      "Accuracy score for test set is 0.8716666666666667\n",
      "Fitting model with epochs = 50, learning rate = 0.003\n",
      "Epoch: 1/50  Loss: 1.0014\n",
      "Epoch: 1/50  Loss: 0.7867\n",
      "Epoch: 1/50  Loss: 0.7548\n",
      "Epoch: 1/50  Loss: 0.6678\n",
      "Epoch: 1/50  Loss: 0.6410\n",
      "Epoch: 1/50  Loss: 0.6303\n",
      "Epoch: 1/50  Loss: 0.6517\n",
      "Epoch: 1/50  Loss: 0.5988\n",
      "Epoch: 1/50  Loss: 0.5587\n",
      "Epoch: 1/50  Loss: 0.5540\n",
      "Epoch: 2/50  Loss: 0.5132\n",
      "Epoch: 2/50  Loss: 0.5142\n",
      "Epoch: 2/50  Loss: 0.5258\n",
      "Epoch: 2/50  Loss: 0.4485\n",
      "Epoch: 2/50  Loss: 0.5023\n",
      "Epoch: 2/50  Loss: 0.5039\n",
      "Epoch: 2/50  Loss: 0.4972\n",
      "Epoch: 2/50  Loss: 0.5229\n",
      "Epoch: 2/50  Loss: 0.4502\n",
      "Epoch: 2/50  Loss: 0.5071\n",
      "Epoch: 3/50  Loss: 0.4362\n",
      "Epoch: 3/50  Loss: 0.3863\n",
      "Epoch: 3/50  Loss: 0.4150\n",
      "Epoch: 3/50  Loss: 0.4186\n",
      "Epoch: 3/50  Loss: 0.4410\n",
      "Epoch: 3/50  Loss: 0.4465\n",
      "Epoch: 3/50  Loss: 0.4211\n",
      "Epoch: 3/50  Loss: 0.4321\n",
      "Epoch: 3/50  Loss: 0.4199\n",
      "Epoch: 3/50  Loss: 0.4160\n",
      "Epoch: 4/50  Loss: 0.3922\n",
      "Epoch: 4/50  Loss: 0.3475\n",
      "Epoch: 4/50  Loss: 0.4073\n",
      "Epoch: 4/50  Loss: 0.3564\n",
      "Epoch: 4/50  Loss: 0.3485\n",
      "Epoch: 4/50  Loss: 0.3573\n",
      "Epoch: 4/50  Loss: 0.3903\n",
      "Epoch: 4/50  Loss: 0.3824\n",
      "Epoch: 4/50  Loss: 0.3481\n",
      "Epoch: 4/50  Loss: 0.3915\n",
      "Epoch: 5/50  Loss: 0.3136\n",
      "Epoch: 5/50  Loss: 0.2939\n",
      "Epoch: 5/50  Loss: 0.3378\n",
      "Epoch: 5/50  Loss: 0.2899\n",
      "Epoch: 5/50  Loss: 0.3463\n",
      "Epoch: 5/50  Loss: 0.3647\n",
      "Epoch: 5/50  Loss: 0.3159\n",
      "Epoch: 5/50  Loss: 0.3649\n",
      "Epoch: 5/50  Loss: 0.3590\n",
      "Epoch: 5/50  Loss: 0.3122\n",
      "Epoch: 6/50  Loss: 0.2484\n",
      "Epoch: 6/50  Loss: 0.2730\n",
      "Epoch: 6/50  Loss: 0.2907\n",
      "Epoch: 6/50  Loss: 0.2838\n",
      "Epoch: 6/50  Loss: 0.2832\n",
      "Epoch: 6/50  Loss: 0.2795\n",
      "Epoch: 6/50  Loss: 0.2949\n",
      "Epoch: 6/50  Loss: 0.3072\n",
      "Epoch: 6/50  Loss: 0.3035\n",
      "Epoch: 6/50  Loss: 0.2988\n",
      "Epoch: 7/50  Loss: 0.2222\n",
      "Epoch: 7/50  Loss: 0.2328\n",
      "Epoch: 7/50  Loss: 0.2648\n",
      "Epoch: 7/50  Loss: 0.2330\n",
      "Epoch: 7/50  Loss: 0.2645\n",
      "Epoch: 7/50  Loss: 0.2898\n",
      "Epoch: 7/50  Loss: 0.2729\n",
      "Epoch: 7/50  Loss: 0.2627\n",
      "Epoch: 7/50  Loss: 0.2455\n",
      "Epoch: 7/50  Loss: 0.2569\n",
      "Epoch: 8/50  Loss: 0.2062\n",
      "Epoch: 8/50  Loss: 0.2094\n",
      "Epoch: 8/50  Loss: 0.1911\n",
      "Epoch: 8/50  Loss: 0.2354\n",
      "Epoch: 8/50  Loss: 0.2401\n",
      "Epoch: 8/50  Loss: 0.2475\n",
      "Epoch: 8/50  Loss: 0.2058\n",
      "Epoch: 8/50  Loss: 0.2647\n",
      "Epoch: 8/50  Loss: 0.2344\n",
      "Epoch: 8/50  Loss: 0.2367\n",
      "Epoch: 9/50  Loss: 0.1919\n",
      "Epoch: 9/50  Loss: 0.1807\n",
      "Epoch: 9/50  Loss: 0.1851\n",
      "Epoch: 9/50  Loss: 0.1752\n",
      "Epoch: 9/50  Loss: 0.1917\n",
      "Epoch: 9/50  Loss: 0.2403\n",
      "Epoch: 9/50  Loss: 0.2254\n",
      "Epoch: 9/50  Loss: 0.1898\n",
      "Epoch: 9/50  Loss: 0.2366\n",
      "Epoch: 9/50  Loss: 0.2389\n",
      "Epoch: 10/50  Loss: 0.1750\n",
      "Epoch: 10/50  Loss: 0.1553\n",
      "Epoch: 10/50  Loss: 0.1639\n",
      "Epoch: 10/50  Loss: 0.1800\n",
      "Epoch: 10/50  Loss: 0.1890\n",
      "Epoch: 10/50  Loss: 0.1774\n",
      "Epoch: 10/50  Loss: 0.1659\n",
      "Epoch: 10/50  Loss: 0.2130\n",
      "Epoch: 10/50  Loss: 0.2311\n",
      "Epoch: 10/50  Loss: 0.2084\n",
      "Epoch: 11/50  Loss: 0.1564\n",
      "Epoch: 11/50  Loss: 0.1346\n",
      "Epoch: 11/50  Loss: 0.1593\n",
      "Epoch: 11/50  Loss: 0.1912\n",
      "Epoch: 11/50  Loss: 0.1799\n",
      "Epoch: 11/50  Loss: 0.1474\n",
      "Epoch: 11/50  Loss: 0.1536\n",
      "Epoch: 11/50  Loss: 0.1666\n",
      "Epoch: 11/50  Loss: 0.2153\n",
      "Epoch: 11/50  Loss: 0.1896\n",
      "Epoch: 12/50  Loss: 0.1336\n",
      "Epoch: 12/50  Loss: 0.1361\n",
      "Epoch: 12/50  Loss: 0.1435\n",
      "Epoch: 12/50  Loss: 0.1406\n",
      "Epoch: 12/50  Loss: 0.1598\n",
      "Epoch: 12/50  Loss: 0.1536\n",
      "Epoch: 12/50  Loss: 0.1687\n",
      "Epoch: 12/50  Loss: 0.1737\n",
      "Epoch: 12/50  Loss: 0.1602\n",
      "Epoch: 12/50  Loss: 0.1585\n",
      "Epoch: 13/50  Loss: 0.1175\n",
      "Epoch: 13/50  Loss: 0.1081\n",
      "Epoch: 13/50  Loss: 0.1546\n",
      "Epoch: 13/50  Loss: 0.1286\n",
      "Epoch: 13/50  Loss: 0.1162\n",
      "Epoch: 13/50  Loss: 0.1470\n",
      "Epoch: 13/50  Loss: 0.1236\n",
      "Epoch: 13/50  Loss: 0.1612\n",
      "Epoch: 13/50  Loss: 0.1680\n",
      "Epoch: 13/50  Loss: 0.1585\n",
      "Epoch: 14/50  Loss: 0.1265\n",
      "Epoch: 14/50  Loss: 0.1031\n",
      "Epoch: 14/50  Loss: 0.1177\n",
      "Epoch: 14/50  Loss: 0.1420\n",
      "Epoch: 14/50  Loss: 0.1391\n",
      "Epoch: 14/50  Loss: 0.1571\n",
      "Epoch: 14/50  Loss: 0.1147\n",
      "Epoch: 14/50  Loss: 0.1629\n",
      "Epoch: 14/50  Loss: 0.1106\n",
      "Epoch: 14/50  Loss: 0.1669\n",
      "Epoch: 15/50  Loss: 0.1091\n",
      "Epoch: 15/50  Loss: 0.0907\n",
      "Epoch: 15/50  Loss: 0.1216\n",
      "Epoch: 15/50  Loss: 0.1123\n",
      "Epoch: 15/50  Loss: 0.1282\n",
      "Epoch: 15/50  Loss: 0.1082\n",
      "Epoch: 15/50  Loss: 0.1385\n",
      "Epoch: 15/50  Loss: 0.1513\n",
      "Epoch: 15/50  Loss: 0.1280\n",
      "Epoch: 15/50  Loss: 0.1408\n",
      "Epoch: 16/50  Loss: 0.0915\n",
      "Epoch: 16/50  Loss: 0.1053\n",
      "Epoch: 16/50  Loss: 0.1113\n",
      "Epoch: 16/50  Loss: 0.0977\n",
      "Epoch: 16/50  Loss: 0.1107\n",
      "Epoch: 16/50  Loss: 0.1151\n",
      "Epoch: 16/50  Loss: 0.0963\n",
      "Epoch: 16/50  Loss: 0.1245\n",
      "Epoch: 16/50  Loss: 0.1355\n",
      "Epoch: 16/50  Loss: 0.1341\n",
      "Epoch: 17/50  Loss: 0.0997\n",
      "Epoch: 17/50  Loss: 0.0948\n",
      "Epoch: 17/50  Loss: 0.0820\n",
      "Epoch: 17/50  Loss: 0.1128\n",
      "Epoch: 17/50  Loss: 0.1102\n",
      "Epoch: 17/50  Loss: 0.1102\n",
      "Epoch: 17/50  Loss: 0.1303\n",
      "Epoch: 17/50  Loss: 0.1090\n",
      "Epoch: 17/50  Loss: 0.0861\n",
      "Epoch: 17/50  Loss: 0.1031\n",
      "Epoch: 18/50  Loss: 0.0803\n",
      "Epoch: 18/50  Loss: 0.0826\n",
      "Epoch: 18/50  Loss: 0.0695\n",
      "Epoch: 18/50  Loss: 0.0825\n",
      "Epoch: 18/50  Loss: 0.0839\n",
      "Epoch: 18/50  Loss: 0.1032\n",
      "Epoch: 18/50  Loss: 0.1049\n",
      "Epoch: 18/50  Loss: 0.1090\n",
      "Epoch: 18/50  Loss: 0.1032\n",
      "Epoch: 18/50  Loss: 0.1142\n",
      "Epoch: 19/50  Loss: 0.0691\n",
      "Epoch: 19/50  Loss: 0.0796\n",
      "Epoch: 19/50  Loss: 0.1075\n",
      "Epoch: 19/50  Loss: 0.0888\n",
      "Epoch: 19/50  Loss: 0.1044\n",
      "Epoch: 19/50  Loss: 0.0981\n",
      "Epoch: 19/50  Loss: 0.1109\n",
      "Epoch: 19/50  Loss: 0.1079\n",
      "Epoch: 19/50  Loss: 0.1022\n",
      "Epoch: 19/50  Loss: 0.1015\n",
      "Epoch: 20/50  Loss: 0.0708\n",
      "Epoch: 20/50  Loss: 0.1024\n",
      "Epoch: 20/50  Loss: 0.0900\n",
      "Epoch: 20/50  Loss: 0.0954\n",
      "Epoch: 20/50  Loss: 0.1040\n",
      "Epoch: 20/50  Loss: 0.0990\n",
      "Epoch: 20/50  Loss: 0.1152\n",
      "Epoch: 20/50  Loss: 0.0832\n",
      "Epoch: 20/50  Loss: 0.0817\n",
      "Epoch: 20/50  Loss: 0.1016\n",
      "Epoch: 21/50  Loss: 0.0836\n",
      "Epoch: 21/50  Loss: 0.0644\n",
      "Epoch: 21/50  Loss: 0.0740\n",
      "Epoch: 21/50  Loss: 0.0606\n",
      "Epoch: 21/50  Loss: 0.0730\n",
      "Epoch: 21/50  Loss: 0.1075\n",
      "Epoch: 21/50  Loss: 0.0780\n",
      "Epoch: 21/50  Loss: 0.0742\n",
      "Epoch: 21/50  Loss: 0.0748\n",
      "Epoch: 21/50  Loss: 0.0922\n",
      "Epoch: 22/50  Loss: 0.0754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22/50  Loss: 0.0637\n",
      "Epoch: 22/50  Loss: 0.0641\n",
      "Epoch: 22/50  Loss: 0.0816\n",
      "Epoch: 22/50  Loss: 0.0798\n",
      "Epoch: 22/50  Loss: 0.0736\n",
      "Epoch: 22/50  Loss: 0.0926\n",
      "Epoch: 22/50  Loss: 0.0956\n",
      "Epoch: 22/50  Loss: 0.0897\n",
      "Epoch: 22/50  Loss: 0.0859\n",
      "Epoch: 23/50  Loss: 0.0670\n",
      "Epoch: 23/50  Loss: 0.0629\n",
      "Epoch: 23/50  Loss: 0.0866\n",
      "Epoch: 23/50  Loss: 0.0656\n",
      "Epoch: 23/50  Loss: 0.0842\n",
      "Epoch: 23/50  Loss: 0.0527\n",
      "Epoch: 23/50  Loss: 0.0813\n",
      "Epoch: 23/50  Loss: 0.0930\n",
      "Epoch: 23/50  Loss: 0.0827\n",
      "Epoch: 23/50  Loss: 0.0908\n",
      "Epoch: 24/50  Loss: 0.0600\n",
      "Epoch: 24/50  Loss: 0.0454\n",
      "Epoch: 24/50  Loss: 0.0595\n",
      "Epoch: 24/50  Loss: 0.0562\n",
      "Epoch: 24/50  Loss: 0.0709\n",
      "Epoch: 24/50  Loss: 0.0690\n",
      "Epoch: 24/50  Loss: 0.0744\n",
      "Epoch: 24/50  Loss: 0.0574\n",
      "Epoch: 24/50  Loss: 0.0770\n",
      "Epoch: 24/50  Loss: 0.0905\n",
      "Epoch: 25/50  Loss: 0.0511\n",
      "Epoch: 25/50  Loss: 0.0576\n",
      "Epoch: 25/50  Loss: 0.0796\n",
      "Epoch: 25/50  Loss: 0.0594\n",
      "Epoch: 25/50  Loss: 0.0641\n",
      "Epoch: 25/50  Loss: 0.0710\n",
      "Epoch: 25/50  Loss: 0.0879\n",
      "Epoch: 25/50  Loss: 0.0672\n",
      "Epoch: 25/50  Loss: 0.0813\n",
      "Epoch: 25/50  Loss: 0.0927\n",
      "Epoch: 26/50  Loss: 0.0525\n",
      "Epoch: 26/50  Loss: 0.0694\n",
      "Epoch: 26/50  Loss: 0.0525\n",
      "Epoch: 26/50  Loss: 0.0531\n",
      "Epoch: 26/50  Loss: 0.0679\n",
      "Epoch: 26/50  Loss: 0.0654\n",
      "Epoch: 26/50  Loss: 0.0540\n",
      "Epoch: 26/50  Loss: 0.0767\n",
      "Epoch: 26/50  Loss: 0.0741\n",
      "Epoch: 26/50  Loss: 0.0757\n",
      "Epoch: 27/50  Loss: 0.0459\n",
      "Epoch: 27/50  Loss: 0.0577\n",
      "Epoch: 27/50  Loss: 0.0527\n",
      "Epoch: 27/50  Loss: 0.0661\n",
      "Epoch: 27/50  Loss: 0.0593\n",
      "Epoch: 27/50  Loss: 0.0591\n",
      "Epoch: 27/50  Loss: 0.0632\n",
      "Epoch: 27/50  Loss: 0.0537\n",
      "Epoch: 27/50  Loss: 0.0672\n",
      "Epoch: 27/50  Loss: 0.0677\n",
      "Epoch: 28/50  Loss: 0.0596\n",
      "Epoch: 28/50  Loss: 0.0448\n",
      "Epoch: 28/50  Loss: 0.0494\n",
      "Epoch: 28/50  Loss: 0.0541\n",
      "Epoch: 28/50  Loss: 0.0713\n",
      "Epoch: 28/50  Loss: 0.0654\n",
      "Epoch: 28/50  Loss: 0.0644\n",
      "Epoch: 28/50  Loss: 0.0589\n",
      "Epoch: 28/50  Loss: 0.0705\n",
      "Epoch: 28/50  Loss: 0.0654\n",
      "Epoch: 29/50  Loss: 0.0491\n",
      "Epoch: 29/50  Loss: 0.0485\n",
      "Epoch: 29/50  Loss: 0.0552\n",
      "Epoch: 29/50  Loss: 0.0515\n",
      "Epoch: 29/50  Loss: 0.0700\n",
      "Epoch: 29/50  Loss: 0.0635\n",
      "Epoch: 29/50  Loss: 0.0702\n",
      "Epoch: 29/50  Loss: 0.0555\n",
      "Epoch: 29/50  Loss: 0.0795\n",
      "Epoch: 29/50  Loss: 0.0699\n",
      "Epoch: 30/50  Loss: 0.0496\n",
      "Epoch: 30/50  Loss: 0.0577\n",
      "Epoch: 30/50  Loss: 0.0444\n",
      "Epoch: 30/50  Loss: 0.0474\n",
      "Epoch: 30/50  Loss: 0.0580\n",
      "Epoch: 30/50  Loss: 0.0561\n",
      "Epoch: 30/50  Loss: 0.0588\n",
      "Epoch: 30/50  Loss: 0.0569\n",
      "Epoch: 30/50  Loss: 0.0678\n",
      "Epoch: 30/50  Loss: 0.0698\n",
      "Epoch: 31/50  Loss: 0.0418\n",
      "Epoch: 31/50  Loss: 0.0477\n",
      "Epoch: 31/50  Loss: 0.0649\n",
      "Epoch: 31/50  Loss: 0.0495\n",
      "Epoch: 31/50  Loss: 0.0393\n",
      "Epoch: 31/50  Loss: 0.0495\n",
      "Epoch: 31/50  Loss: 0.0660\n",
      "Epoch: 31/50  Loss: 0.0567\n",
      "Epoch: 31/50  Loss: 0.0375\n",
      "Epoch: 31/50  Loss: 0.0450\n",
      "Epoch: 32/50  Loss: 0.0400\n",
      "Epoch: 32/50  Loss: 0.0440\n",
      "Epoch: 32/50  Loss: 0.0448\n",
      "Epoch: 32/50  Loss: 0.0548\n",
      "Epoch: 32/50  Loss: 0.0409\n",
      "Epoch: 32/50  Loss: 0.0493\n",
      "Epoch: 32/50  Loss: 0.0438\n",
      "Epoch: 32/50  Loss: 0.0546\n",
      "Epoch: 32/50  Loss: 0.0445\n",
      "Epoch: 32/50  Loss: 0.0466\n",
      "Epoch: 33/50  Loss: 0.0586\n",
      "Epoch: 33/50  Loss: 0.0416\n",
      "Epoch: 33/50  Loss: 0.0326\n",
      "Epoch: 33/50  Loss: 0.0668\n",
      "Epoch: 33/50  Loss: 0.0650\n",
      "Epoch: 33/50  Loss: 0.0408\n",
      "Epoch: 33/50  Loss: 0.0688\n",
      "Epoch: 33/50  Loss: 0.0605\n",
      "Epoch: 33/50  Loss: 0.0615\n",
      "Epoch: 33/50  Loss: 0.0575\n",
      "Epoch: 34/50  Loss: 0.0411\n",
      "Epoch: 34/50  Loss: 0.0309\n",
      "Epoch: 34/50  Loss: 0.0418\n",
      "Epoch: 34/50  Loss: 0.0352\n",
      "Epoch: 34/50  Loss: 0.0381\n",
      "Epoch: 34/50  Loss: 0.0348\n",
      "Epoch: 34/50  Loss: 0.0490\n",
      "Epoch: 34/50  Loss: 0.0472\n",
      "Epoch: 34/50  Loss: 0.0541\n",
      "Epoch: 34/50  Loss: 0.0510\n",
      "Epoch: 35/50  Loss: 0.0473\n",
      "Epoch: 35/50  Loss: 0.0544\n",
      "Epoch: 35/50  Loss: 0.0403\n",
      "Epoch: 35/50  Loss: 0.0479\n",
      "Epoch: 35/50  Loss: 0.0497\n",
      "Epoch: 35/50  Loss: 0.0590\n",
      "Epoch: 35/50  Loss: 0.0623\n",
      "Epoch: 35/50  Loss: 0.0493\n",
      "Epoch: 35/50  Loss: 0.0456\n",
      "Epoch: 35/50  Loss: 0.0428\n",
      "Epoch: 36/50  Loss: 0.0266\n",
      "Epoch: 36/50  Loss: 0.0230\n",
      "Epoch: 36/50  Loss: 0.0277\n",
      "Epoch: 36/50  Loss: 0.0319\n",
      "Epoch: 36/50  Loss: 0.0318\n",
      "Epoch: 36/50  Loss: 0.0402\n",
      "Epoch: 36/50  Loss: 0.0678\n",
      "Epoch: 36/50  Loss: 0.0678\n",
      "Epoch: 36/50  Loss: 0.0394\n",
      "Epoch: 36/50  Loss: 0.0623\n",
      "Epoch: 37/50  Loss: 0.0375\n",
      "Epoch: 37/50  Loss: 0.0422\n",
      "Epoch: 37/50  Loss: 0.0407\n",
      "Epoch: 37/50  Loss: 0.0484\n",
      "Epoch: 37/50  Loss: 0.0333\n",
      "Epoch: 37/50  Loss: 0.0652\n",
      "Epoch: 37/50  Loss: 0.0494\n",
      "Epoch: 37/50  Loss: 0.0629\n",
      "Epoch: 37/50  Loss: 0.0500\n",
      "Epoch: 37/50  Loss: 0.0438\n",
      "Epoch: 38/50  Loss: 0.0274\n",
      "Epoch: 38/50  Loss: 0.0385\n",
      "Epoch: 38/50  Loss: 0.0459\n",
      "Epoch: 38/50  Loss: 0.0359\n",
      "Epoch: 38/50  Loss: 0.0341\n",
      "Epoch: 38/50  Loss: 0.0332\n",
      "Epoch: 38/50  Loss: 0.0469\n",
      "Epoch: 38/50  Loss: 0.0515\n",
      "Epoch: 38/50  Loss: 0.0522\n",
      "Epoch: 38/50  Loss: 0.0425\n",
      "Epoch: 39/50  Loss: 0.0371\n",
      "Epoch: 39/50  Loss: 0.0506\n",
      "Epoch: 39/50  Loss: 0.0468\n",
      "Epoch: 39/50  Loss: 0.0485\n",
      "Epoch: 39/50  Loss: 0.0263\n",
      "Epoch: 39/50  Loss: 0.0418\n",
      "Epoch: 39/50  Loss: 0.0499\n",
      "Epoch: 39/50  Loss: 0.0521\n",
      "Epoch: 39/50  Loss: 0.0481\n",
      "Epoch: 39/50  Loss: 0.0551\n",
      "Epoch: 40/50  Loss: 0.0337\n",
      "Epoch: 40/50  Loss: 0.0322\n",
      "Epoch: 40/50  Loss: 0.0407\n",
      "Epoch: 40/50  Loss: 0.0362\n",
      "Epoch: 40/50  Loss: 0.0361\n",
      "Epoch: 40/50  Loss: 0.0413\n",
      "Epoch: 40/50  Loss: 0.0419\n",
      "Epoch: 40/50  Loss: 0.0479\n",
      "Epoch: 40/50  Loss: 0.0469\n",
      "Epoch: 40/50  Loss: 0.0391\n",
      "Epoch: 41/50  Loss: 0.0327\n",
      "Epoch: 41/50  Loss: 0.0379\n",
      "Epoch: 41/50  Loss: 0.0321\n",
      "Epoch: 41/50  Loss: 0.0304\n",
      "Epoch: 41/50  Loss: 0.0419\n",
      "Epoch: 41/50  Loss: 0.0443\n",
      "Epoch: 41/50  Loss: 0.0353\n",
      "Epoch: 41/50  Loss: 0.0331\n",
      "Epoch: 41/50  Loss: 0.0535\n",
      "Epoch: 41/50  Loss: 0.0507\n",
      "Epoch: 42/50  Loss: 0.0344\n",
      "Epoch: 42/50  Loss: 0.0294\n",
      "Epoch: 42/50  Loss: 0.0327\n",
      "Epoch: 42/50  Loss: 0.0345\n",
      "Epoch: 42/50  Loss: 0.0365\n",
      "Epoch: 42/50  Loss: 0.0336\n",
      "Epoch: 42/50  Loss: 0.0357\n",
      "Epoch: 42/50  Loss: 0.0477\n",
      "Epoch: 42/50  Loss: 0.0453\n",
      "Epoch: 42/50  Loss: 0.0310\n",
      "Epoch: 43/50  Loss: 0.0171\n",
      "Epoch: 43/50  Loss: 0.0310\n",
      "Epoch: 43/50  Loss: 0.0383\n",
      "Epoch: 43/50  Loss: 0.0293\n",
      "Epoch: 43/50  Loss: 0.0503\n",
      "Epoch: 43/50  Loss: 0.0408\n",
      "Epoch: 43/50  Loss: 0.0317\n",
      "Epoch: 43/50  Loss: 0.0455\n",
      "Epoch: 43/50  Loss: 0.0484\n",
      "Epoch: 43/50  Loss: 0.0426\n",
      "Epoch: 44/50  Loss: 0.0213\n",
      "Epoch: 44/50  Loss: 0.0275\n",
      "Epoch: 44/50  Loss: 0.0413\n",
      "Epoch: 44/50  Loss: 0.0334\n",
      "Epoch: 44/50  Loss: 0.0319\n",
      "Epoch: 44/50  Loss: 0.0343\n",
      "Epoch: 44/50  Loss: 0.0475\n",
      "Epoch: 44/50  Loss: 0.0557\n",
      "Epoch: 44/50  Loss: 0.0438\n",
      "Epoch: 44/50  Loss: 0.0538\n",
      "Epoch: 45/50  Loss: 0.0336\n",
      "Epoch: 45/50  Loss: 0.0253\n",
      "Epoch: 45/50  Loss: 0.0161\n",
      "Epoch: 45/50  Loss: 0.0377\n",
      "Epoch: 45/50  Loss: 0.0433\n",
      "Epoch: 45/50  Loss: 0.0371\n",
      "Epoch: 45/50  Loss: 0.0305\n",
      "Epoch: 45/50  Loss: 0.0404\n",
      "Epoch: 45/50  Loss: 0.0383\n",
      "Epoch: 45/50  Loss: 0.0428\n",
      "Epoch: 46/50  Loss: 0.0274\n",
      "Epoch: 46/50  Loss: 0.0172\n",
      "Epoch: 46/50  Loss: 0.0279\n",
      "Epoch: 46/50  Loss: 0.0261\n",
      "Epoch: 46/50  Loss: 0.0484\n",
      "Epoch: 46/50  Loss: 0.0503\n",
      "Epoch: 46/50  Loss: 0.0395\n",
      "Epoch: 46/50  Loss: 0.0342\n",
      "Epoch: 46/50  Loss: 0.0320\n",
      "Epoch: 46/50  Loss: 0.0292\n",
      "Epoch: 47/50  Loss: 0.0154\n",
      "Epoch: 47/50  Loss: 0.0252\n",
      "Epoch: 47/50  Loss: 0.0211\n",
      "Epoch: 47/50  Loss: 0.0299\n",
      "Epoch: 47/50  Loss: 0.0311\n",
      "Epoch: 47/50  Loss: 0.0360\n",
      "Epoch: 47/50  Loss: 0.0347\n",
      "Epoch: 47/50  Loss: 0.0398\n",
      "Epoch: 47/50  Loss: 0.0332\n",
      "Epoch: 47/50  Loss: 0.0308\n",
      "Epoch: 48/50  Loss: 0.0194\n",
      "Epoch: 48/50  Loss: 0.0246\n",
      "Epoch: 48/50  Loss: 0.0486\n",
      "Epoch: 48/50  Loss: 0.0306\n",
      "Epoch: 48/50  Loss: 0.0385\n",
      "Epoch: 48/50  Loss: 0.0571\n",
      "Epoch: 48/50  Loss: 0.0518\n",
      "Epoch: 48/50  Loss: 0.0299\n",
      "Epoch: 48/50  Loss: 0.0364\n",
      "Epoch: 48/50  Loss: 0.0369\n",
      "Epoch: 49/50  Loss: 0.0336\n",
      "Epoch: 49/50  Loss: 0.0242\n",
      "Epoch: 49/50  Loss: 0.0232\n",
      "Epoch: 49/50  Loss: 0.0235\n",
      "Epoch: 49/50  Loss: 0.0249\n",
      "Epoch: 49/50  Loss: 0.0404\n",
      "Epoch: 49/50  Loss: 0.0397\n",
      "Epoch: 49/50  Loss: 0.0261\n",
      "Epoch: 49/50  Loss: 0.0379\n",
      "Epoch: 49/50  Loss: 0.0495\n",
      "Epoch: 50/50  Loss: 0.0320\n",
      "Epoch: 50/50  Loss: 0.0283\n",
      "Epoch: 50/50  Loss: 0.0342\n",
      "Epoch: 50/50  Loss: 0.0279\n",
      "Epoch: 50/50  Loss: 0.0334\n",
      "Epoch: 50/50  Loss: 0.0296\n",
      "Epoch: 50/50  Loss: 0.0429\n",
      "Epoch: 50/50  Loss: 0.0208\n",
      "Epoch: 50/50  Loss: 0.0381\n",
      "Epoch: 50/50  Loss: 0.0536\n",
      "Accuracy score for train set is 0.9971904761904762\n",
      "Accuracy score for test set is 0.8751111111111111\n",
      "Fitting model with epochs = 60, learning rate = 0.003\n",
      "Epoch: 1/60  Loss: 1.0465\n",
      "Epoch: 1/60  Loss: 0.7600\n",
      "Epoch: 1/60  Loss: 0.7548\n",
      "Epoch: 1/60  Loss: 0.7002\n",
      "Epoch: 1/60  Loss: 0.5973\n",
      "Epoch: 1/60  Loss: 0.6014\n",
      "Epoch: 1/60  Loss: 0.5967\n",
      "Epoch: 1/60  Loss: 0.6129\n",
      "Epoch: 1/60  Loss: 0.5629\n",
      "Epoch: 1/60  Loss: 0.5581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/60  Loss: 0.4898\n",
      "Epoch: 2/60  Loss: 0.4762\n",
      "Epoch: 2/60  Loss: 0.5007\n",
      "Epoch: 2/60  Loss: 0.4915\n",
      "Epoch: 2/60  Loss: 0.5418\n",
      "Epoch: 2/60  Loss: 0.5248\n",
      "Epoch: 2/60  Loss: 0.4849\n",
      "Epoch: 2/60  Loss: 0.4953\n",
      "Epoch: 2/60  Loss: 0.4631\n",
      "Epoch: 2/60  Loss: 0.4784\n",
      "Epoch: 3/60  Loss: 0.4175\n",
      "Epoch: 3/60  Loss: 0.4298\n",
      "Epoch: 3/60  Loss: 0.4155\n",
      "Epoch: 3/60  Loss: 0.4265\n",
      "Epoch: 3/60  Loss: 0.3921\n",
      "Epoch: 3/60  Loss: 0.4349\n",
      "Epoch: 3/60  Loss: 0.4481\n",
      "Epoch: 3/60  Loss: 0.4650\n",
      "Epoch: 3/60  Loss: 0.4379\n",
      "Epoch: 3/60  Loss: 0.3912\n",
      "Epoch: 4/60  Loss: 0.3583\n",
      "Epoch: 4/60  Loss: 0.3607\n",
      "Epoch: 4/60  Loss: 0.3812\n",
      "Epoch: 4/60  Loss: 0.3620\n",
      "Epoch: 4/60  Loss: 0.3975\n",
      "Epoch: 4/60  Loss: 0.3566\n",
      "Epoch: 4/60  Loss: 0.3674\n",
      "Epoch: 4/60  Loss: 0.3945\n",
      "Epoch: 4/60  Loss: 0.3852\n",
      "Epoch: 4/60  Loss: 0.3391\n",
      "Epoch: 5/60  Loss: 0.2685\n",
      "Epoch: 5/60  Loss: 0.3248\n",
      "Epoch: 5/60  Loss: 0.2599\n",
      "Epoch: 5/60  Loss: 0.3102\n",
      "Epoch: 5/60  Loss: 0.2964\n",
      "Epoch: 5/60  Loss: 0.3633\n",
      "Epoch: 5/60  Loss: 0.3511\n",
      "Epoch: 5/60  Loss: 0.3472\n",
      "Epoch: 5/60  Loss: 0.3547\n",
      "Epoch: 5/60  Loss: 0.3914\n",
      "Epoch: 6/60  Loss: 0.2617\n",
      "Epoch: 6/60  Loss: 0.2760\n",
      "Epoch: 6/60  Loss: 0.2815\n",
      "Epoch: 6/60  Loss: 0.3050\n",
      "Epoch: 6/60  Loss: 0.2768\n",
      "Epoch: 6/60  Loss: 0.3126\n",
      "Epoch: 6/60  Loss: 0.3005\n",
      "Epoch: 6/60  Loss: 0.3113\n",
      "Epoch: 6/60  Loss: 0.3020\n",
      "Epoch: 6/60  Loss: 0.2954\n",
      "Epoch: 7/60  Loss: 0.2353\n",
      "Epoch: 7/60  Loss: 0.2638\n",
      "Epoch: 7/60  Loss: 0.2527\n",
      "Epoch: 7/60  Loss: 0.2412\n",
      "Epoch: 7/60  Loss: 0.2721\n",
      "Epoch: 7/60  Loss: 0.2792\n",
      "Epoch: 7/60  Loss: 0.2562\n",
      "Epoch: 7/60  Loss: 0.2939\n",
      "Epoch: 7/60  Loss: 0.2924\n",
      "Epoch: 7/60  Loss: 0.2827\n",
      "Epoch: 8/60  Loss: 0.2038\n",
      "Epoch: 8/60  Loss: 0.2047\n",
      "Epoch: 8/60  Loss: 0.2569\n",
      "Epoch: 8/60  Loss: 0.2452\n",
      "Epoch: 8/60  Loss: 0.2339\n",
      "Epoch: 8/60  Loss: 0.2421\n",
      "Epoch: 8/60  Loss: 0.2837\n",
      "Epoch: 8/60  Loss: 0.2419\n",
      "Epoch: 8/60  Loss: 0.2460\n",
      "Epoch: 8/60  Loss: 0.2828\n",
      "Epoch: 9/60  Loss: 0.2233\n",
      "Epoch: 9/60  Loss: 0.1904\n",
      "Epoch: 9/60  Loss: 0.1712\n",
      "Epoch: 9/60  Loss: 0.2279\n",
      "Epoch: 9/60  Loss: 0.2036\n",
      "Epoch: 9/60  Loss: 0.2346\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-a7468812bc33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepochs_range\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mfit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_chunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0maccuracy_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-6ef0520a8d62>\u001b[0m in \u001b[0;36mfit_model\u001b[0;34m(model, X_train, y_train, epochs, n_chunks, learning_rate, weight_decay)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs_range = np.arange(10, 150, 10)\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "\n",
    "for epochs in epochs_range:\n",
    "    model = build_model(input_size, output_size, hidden_sizes)\n",
    "    fit_model(model, train, train_labels, epochs = epochs, n_chunks = n_chunks, learning_rate = learning_rate, weight_decay = weight_decay)\n",
    "    accuracy_train, accuracy_test = evaluate_model(model, train, train_labels, test, test_labels)\n",
    "\n",
    "    train_acc.append(accuracy_train)\n",
    "    test_acc.append(accuracy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (14,) and (5,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-933810087ee8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs_range\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'upper left'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy, learning_rate = '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2793\u001b[0m     return gca().plot(\n\u001b[1;32m   2794\u001b[0m         *args, scalex=scalex, scaley=scaley, **({\"data\": data} if data\n\u001b[0;32m-> 2795\u001b[0;31m         is not None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1664\u001b[0m         \"\"\"\n\u001b[1;32m   1665\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_alias_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1666\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1667\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xy_from_xy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'plot'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_xy_from_xy\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             raise ValueError(\"x and y must have same first dimension, but \"\n\u001b[0;32m--> 270\u001b[0;31m                              \"have shapes {} and {}\".format(x.shape, y.shape))\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             raise ValueError(\"x and y can be no greater than 2-D, but have \"\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (14,) and (5,)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = epochs_range\n",
    "plt.plot(x, train_acc)\n",
    "plt.plot(x, test_acc)\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.title('Accuracy, learning_rate = ' + str(learning_rate), fontsize=20)\n",
    "plt.xlabel('Number of epochs', fontsize=14)\n",
    "plt.ylabel('Accuracy', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "n_chunks = 1000\n",
    "learning_rate = 0.003\n",
    "weight_decay = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model with epochs = 100, learning rate = 0.003\n",
      "Epoch: 1/100  Loss: 1.0080\n",
      "Epoch: 1/100  Loss: 0.7703\n",
      "Epoch: 1/100  Loss: 0.7212\n",
      "Epoch: 1/100  Loss: 0.6454\n",
      "Epoch: 1/100  Loss: 0.6657\n",
      "Epoch: 1/100  Loss: 0.6524\n",
      "Epoch: 1/100  Loss: 0.6223\n",
      "Epoch: 1/100  Loss: 0.6299\n",
      "Epoch: 1/100  Loss: 0.5653\n",
      "Epoch: 1/100  Loss: 0.5683\n",
      "Epoch: 2/100  Loss: 0.4849\n",
      "Epoch: 2/100  Loss: 0.5065\n",
      "Epoch: 2/100  Loss: 0.4869\n",
      "Epoch: 2/100  Loss: 0.4843\n",
      "Epoch: 2/100  Loss: 0.5363\n",
      "Epoch: 2/100  Loss: 0.4564\n",
      "Epoch: 2/100  Loss: 0.5016\n",
      "Epoch: 2/100  Loss: 0.4969\n",
      "Epoch: 2/100  Loss: 0.4650\n",
      "Epoch: 2/100  Loss: 0.5128\n",
      "Epoch: 3/100  Loss: 0.4031\n",
      "Epoch: 3/100  Loss: 0.3917\n",
      "Epoch: 3/100  Loss: 0.4159\n",
      "Epoch: 3/100  Loss: 0.4025\n",
      "Epoch: 3/100  Loss: 0.4463\n",
      "Epoch: 3/100  Loss: 0.4465\n",
      "Epoch: 3/100  Loss: 0.4483\n",
      "Epoch: 3/100  Loss: 0.4246\n",
      "Epoch: 3/100  Loss: 0.4129\n",
      "Epoch: 3/100  Loss: 0.4467\n",
      "Epoch: 4/100  Loss: 0.3505\n",
      "Epoch: 4/100  Loss: 0.3404\n",
      "Epoch: 4/100  Loss: 0.3779\n",
      "Epoch: 4/100  Loss: 0.3786\n",
      "Epoch: 4/100  Loss: 0.3794\n",
      "Epoch: 4/100  Loss: 0.4181\n",
      "Epoch: 4/100  Loss: 0.3698\n",
      "Epoch: 4/100  Loss: 0.3978\n",
      "Epoch: 4/100  Loss: 0.3541\n",
      "Epoch: 4/100  Loss: 0.3547\n",
      "Epoch: 5/100  Loss: 0.3118\n",
      "Epoch: 5/100  Loss: 0.3009\n",
      "Epoch: 5/100  Loss: 0.3260\n",
      "Epoch: 5/100  Loss: 0.3437\n",
      "Epoch: 5/100  Loss: 0.3331\n",
      "Epoch: 5/100  Loss: 0.3628\n",
      "Epoch: 5/100  Loss: 0.2973\n",
      "Epoch: 5/100  Loss: 0.3284\n",
      "Epoch: 5/100  Loss: 0.3395\n",
      "Epoch: 5/100  Loss: 0.3248\n",
      "Epoch: 6/100  Loss: 0.2787\n",
      "Epoch: 6/100  Loss: 0.2509\n",
      "Epoch: 6/100  Loss: 0.2645\n",
      "Epoch: 6/100  Loss: 0.2884\n",
      "Epoch: 6/100  Loss: 0.2903\n",
      "Epoch: 6/100  Loss: 0.2992\n",
      "Epoch: 6/100  Loss: 0.2953\n",
      "Epoch: 6/100  Loss: 0.3034\n",
      "Epoch: 6/100  Loss: 0.2987\n",
      "Epoch: 6/100  Loss: 0.3062\n",
      "Epoch: 7/100  Loss: 0.2268\n",
      "Epoch: 7/100  Loss: 0.2453\n",
      "Epoch: 7/100  Loss: 0.2303\n",
      "Epoch: 7/100  Loss: 0.2266\n",
      "Epoch: 7/100  Loss: 0.2766\n",
      "Epoch: 7/100  Loss: 0.2571\n",
      "Epoch: 7/100  Loss: 0.3144\n",
      "Epoch: 7/100  Loss: 0.2989\n",
      "Epoch: 7/100  Loss: 0.2448\n",
      "Epoch: 7/100  Loss: 0.2649\n",
      "Epoch: 8/100  Loss: 0.1693\n",
      "Epoch: 8/100  Loss: 0.2059\n",
      "Epoch: 8/100  Loss: 0.2153\n",
      "Epoch: 8/100  Loss: 0.2310\n",
      "Epoch: 8/100  Loss: 0.2397\n",
      "Epoch: 8/100  Loss: 0.2612\n",
      "Epoch: 8/100  Loss: 0.2397\n",
      "Epoch: 8/100  Loss: 0.2673\n",
      "Epoch: 8/100  Loss: 0.2634\n",
      "Epoch: 8/100  Loss: 0.2591\n",
      "Epoch: 9/100  Loss: 0.1945\n",
      "Epoch: 9/100  Loss: 0.1902\n",
      "Epoch: 9/100  Loss: 0.1854\n",
      "Epoch: 9/100  Loss: 0.1930\n",
      "Epoch: 9/100  Loss: 0.1899\n",
      "Epoch: 9/100  Loss: 0.2445\n",
      "Epoch: 9/100  Loss: 0.2097\n",
      "Epoch: 9/100  Loss: 0.2103\n",
      "Epoch: 9/100  Loss: 0.2268\n",
      "Epoch: 9/100  Loss: 0.2364\n",
      "Epoch: 10/100  Loss: 0.1533\n",
      "Epoch: 10/100  Loss: 0.1716\n",
      "Epoch: 10/100  Loss: 0.1836\n",
      "Epoch: 10/100  Loss: 0.1827\n",
      "Epoch: 10/100  Loss: 0.1885\n",
      "Epoch: 10/100  Loss: 0.2132\n",
      "Epoch: 10/100  Loss: 0.1497\n",
      "Epoch: 10/100  Loss: 0.2200\n",
      "Epoch: 10/100  Loss: 0.1938\n",
      "Epoch: 10/100  Loss: 0.2318\n",
      "Epoch: 11/100  Loss: 0.1547\n",
      "Epoch: 11/100  Loss: 0.1641\n",
      "Epoch: 11/100  Loss: 0.1782\n",
      "Epoch: 11/100  Loss: 0.1548\n",
      "Epoch: 11/100  Loss: 0.1819\n",
      "Epoch: 11/100  Loss: 0.1608\n",
      "Epoch: 11/100  Loss: 0.1964\n",
      "Epoch: 11/100  Loss: 0.1732\n",
      "Epoch: 11/100  Loss: 0.1585\n",
      "Epoch: 11/100  Loss: 0.1839\n",
      "Epoch: 12/100  Loss: 0.1649\n",
      "Epoch: 12/100  Loss: 0.1231\n",
      "Epoch: 12/100  Loss: 0.1844\n",
      "Epoch: 12/100  Loss: 0.1496\n",
      "Epoch: 12/100  Loss: 0.1439\n",
      "Epoch: 12/100  Loss: 0.1396\n",
      "Epoch: 12/100  Loss: 0.1637\n",
      "Epoch: 12/100  Loss: 0.2091\n",
      "Epoch: 12/100  Loss: 0.1711\n",
      "Epoch: 12/100  Loss: 0.1765\n",
      "Epoch: 13/100  Loss: 0.1292\n",
      "Epoch: 13/100  Loss: 0.1113\n",
      "Epoch: 13/100  Loss: 0.1182\n",
      "Epoch: 13/100  Loss: 0.1255\n",
      "Epoch: 13/100  Loss: 0.1401\n",
      "Epoch: 13/100  Loss: 0.1364\n",
      "Epoch: 13/100  Loss: 0.1267\n",
      "Epoch: 13/100  Loss: 0.1517\n",
      "Epoch: 13/100  Loss: 0.1614\n",
      "Epoch: 13/100  Loss: 0.1533\n",
      "Epoch: 14/100  Loss: 0.0962\n",
      "Epoch: 14/100  Loss: 0.1056\n",
      "Epoch: 14/100  Loss: 0.1126\n",
      "Epoch: 14/100  Loss: 0.1322\n",
      "Epoch: 14/100  Loss: 0.1376\n",
      "Epoch: 14/100  Loss: 0.1319\n",
      "Epoch: 14/100  Loss: 0.1485\n",
      "Epoch: 14/100  Loss: 0.1463\n",
      "Epoch: 14/100  Loss: 0.1561\n",
      "Epoch: 14/100  Loss: 0.1429\n",
      "Epoch: 15/100  Loss: 0.1185\n",
      "Epoch: 15/100  Loss: 0.1151\n",
      "Epoch: 15/100  Loss: 0.1340\n",
      "Epoch: 15/100  Loss: 0.1177\n",
      "Epoch: 15/100  Loss: 0.1023\n",
      "Epoch: 15/100  Loss: 0.1273\n",
      "Epoch: 15/100  Loss: 0.1211\n",
      "Epoch: 15/100  Loss: 0.1353\n",
      "Epoch: 15/100  Loss: 0.1196\n",
      "Epoch: 15/100  Loss: 0.1587\n",
      "Epoch: 16/100  Loss: 0.0975\n",
      "Epoch: 16/100  Loss: 0.0993\n",
      "Epoch: 16/100  Loss: 0.0891\n",
      "Epoch: 16/100  Loss: 0.1015\n",
      "Epoch: 16/100  Loss: 0.1023\n",
      "Epoch: 16/100  Loss: 0.1041\n",
      "Epoch: 16/100  Loss: 0.0896\n",
      "Epoch: 16/100  Loss: 0.1293\n",
      "Epoch: 16/100  Loss: 0.1087\n",
      "Epoch: 16/100  Loss: 0.1228\n",
      "Epoch: 17/100  Loss: 0.0906\n",
      "Epoch: 17/100  Loss: 0.0797\n",
      "Epoch: 17/100  Loss: 0.1016\n",
      "Epoch: 17/100  Loss: 0.0889\n",
      "Epoch: 17/100  Loss: 0.0985\n",
      "Epoch: 17/100  Loss: 0.1158\n",
      "Epoch: 17/100  Loss: 0.1231\n",
      "Epoch: 17/100  Loss: 0.1266\n",
      "Epoch: 17/100  Loss: 0.1068\n",
      "Epoch: 17/100  Loss: 0.1014\n",
      "Epoch: 18/100  Loss: 0.0764\n",
      "Epoch: 18/100  Loss: 0.0747\n",
      "Epoch: 18/100  Loss: 0.0706\n",
      "Epoch: 18/100  Loss: 0.0962\n",
      "Epoch: 18/100  Loss: 0.1280\n",
      "Epoch: 18/100  Loss: 0.1280\n",
      "Epoch: 18/100  Loss: 0.1198\n",
      "Epoch: 18/100  Loss: 0.1405\n",
      "Epoch: 18/100  Loss: 0.1123\n",
      "Epoch: 18/100  Loss: 0.0938\n",
      "Epoch: 19/100  Loss: 0.0814\n",
      "Epoch: 19/100  Loss: 0.0629\n",
      "Epoch: 19/100  Loss: 0.0641\n",
      "Epoch: 19/100  Loss: 0.0723\n",
      "Epoch: 19/100  Loss: 0.0936\n",
      "Epoch: 19/100  Loss: 0.0960\n",
      "Epoch: 19/100  Loss: 0.1002\n",
      "Epoch: 19/100  Loss: 0.0995\n",
      "Epoch: 19/100  Loss: 0.0916\n",
      "Epoch: 19/100  Loss: 0.1156\n",
      "Epoch: 20/100  Loss: 0.0644\n",
      "Epoch: 20/100  Loss: 0.0567\n",
      "Epoch: 20/100  Loss: 0.0538\n",
      "Epoch: 20/100  Loss: 0.0892\n",
      "Epoch: 20/100  Loss: 0.0832\n",
      "Epoch: 20/100  Loss: 0.0807\n",
      "Epoch: 20/100  Loss: 0.0853\n",
      "Epoch: 20/100  Loss: 0.0949\n",
      "Epoch: 20/100  Loss: 0.0815\n",
      "Epoch: 20/100  Loss: 0.0962\n",
      "Epoch: 21/100  Loss: 0.0567\n",
      "Epoch: 21/100  Loss: 0.0661\n",
      "Epoch: 21/100  Loss: 0.0704\n",
      "Epoch: 21/100  Loss: 0.0794\n",
      "Epoch: 21/100  Loss: 0.0653\n",
      "Epoch: 21/100  Loss: 0.0928\n",
      "Epoch: 21/100  Loss: 0.0977\n",
      "Epoch: 21/100  Loss: 0.0784\n",
      "Epoch: 21/100  Loss: 0.0986\n",
      "Epoch: 21/100  Loss: 0.0743\n",
      "Epoch: 22/100  Loss: 0.0489\n",
      "Epoch: 22/100  Loss: 0.0533\n",
      "Epoch: 22/100  Loss: 0.0578\n",
      "Epoch: 22/100  Loss: 0.0645\n",
      "Epoch: 22/100  Loss: 0.0821\n",
      "Epoch: 22/100  Loss: 0.0982\n",
      "Epoch: 22/100  Loss: 0.0607\n",
      "Epoch: 22/100  Loss: 0.0738\n",
      "Epoch: 22/100  Loss: 0.0871\n",
      "Epoch: 22/100  Loss: 0.0929\n",
      "Epoch: 23/100  Loss: 0.0514\n",
      "Epoch: 23/100  Loss: 0.0575\n",
      "Epoch: 23/100  Loss: 0.0575\n",
      "Epoch: 23/100  Loss: 0.0529\n",
      "Epoch: 23/100  Loss: 0.0596\n",
      "Epoch: 23/100  Loss: 0.0601\n",
      "Epoch: 23/100  Loss: 0.0852\n",
      "Epoch: 23/100  Loss: 0.0815\n",
      "Epoch: 23/100  Loss: 0.0720\n",
      "Epoch: 23/100  Loss: 0.0920\n",
      "Epoch: 24/100  Loss: 0.0503\n",
      "Epoch: 24/100  Loss: 0.0623\n",
      "Epoch: 24/100  Loss: 0.0538\n",
      "Epoch: 24/100  Loss: 0.0514\n",
      "Epoch: 24/100  Loss: 0.0588\n",
      "Epoch: 24/100  Loss: 0.0642\n",
      "Epoch: 24/100  Loss: 0.0740\n",
      "Epoch: 24/100  Loss: 0.0791\n",
      "Epoch: 24/100  Loss: 0.0747\n",
      "Epoch: 24/100  Loss: 0.0835\n",
      "Epoch: 25/100  Loss: 0.0795\n",
      "Epoch: 25/100  Loss: 0.0629\n",
      "Epoch: 25/100  Loss: 0.0452\n",
      "Epoch: 25/100  Loss: 0.0484\n",
      "Epoch: 25/100  Loss: 0.0566\n",
      "Epoch: 25/100  Loss: 0.0625\n",
      "Epoch: 25/100  Loss: 0.0727\n",
      "Epoch: 25/100  Loss: 0.0776\n",
      "Epoch: 25/100  Loss: 0.0664\n",
      "Epoch: 25/100  Loss: 0.0579\n",
      "Epoch: 26/100  Loss: 0.0713\n",
      "Epoch: 26/100  Loss: 0.0599\n",
      "Epoch: 26/100  Loss: 0.0490\n",
      "Epoch: 26/100  Loss: 0.0572\n",
      "Epoch: 26/100  Loss: 0.0709\n",
      "Epoch: 26/100  Loss: 0.0634\n",
      "Epoch: 26/100  Loss: 0.0646\n",
      "Epoch: 26/100  Loss: 0.0461\n",
      "Epoch: 26/100  Loss: 0.0670\n",
      "Epoch: 26/100  Loss: 0.0788\n",
      "Epoch: 27/100  Loss: 0.0662\n",
      "Epoch: 27/100  Loss: 0.0389\n",
      "Epoch: 27/100  Loss: 0.0414\n",
      "Epoch: 27/100  Loss: 0.0478\n",
      "Epoch: 27/100  Loss: 0.0703\n",
      "Epoch: 27/100  Loss: 0.0610\n",
      "Epoch: 27/100  Loss: 0.0675\n",
      "Epoch: 27/100  Loss: 0.0660\n",
      "Epoch: 27/100  Loss: 0.0752\n",
      "Epoch: 27/100  Loss: 0.0729\n",
      "Epoch: 28/100  Loss: 0.0427\n",
      "Epoch: 28/100  Loss: 0.0420\n",
      "Epoch: 28/100  Loss: 0.0517\n",
      "Epoch: 28/100  Loss: 0.0677\n",
      "Epoch: 28/100  Loss: 0.0562\n",
      "Epoch: 28/100  Loss: 0.0592\n",
      "Epoch: 28/100  Loss: 0.0905\n",
      "Epoch: 28/100  Loss: 0.0664\n",
      "Epoch: 28/100  Loss: 0.0615\n",
      "Epoch: 28/100  Loss: 0.0540\n",
      "Epoch: 29/100  Loss: 0.0437\n",
      "Epoch: 29/100  Loss: 0.0426\n",
      "Epoch: 29/100  Loss: 0.0543\n",
      "Epoch: 29/100  Loss: 0.0604\n",
      "Epoch: 29/100  Loss: 0.0553\n",
      "Epoch: 29/100  Loss: 0.0520\n",
      "Epoch: 29/100  Loss: 0.0472\n",
      "Epoch: 29/100  Loss: 0.0542\n",
      "Epoch: 29/100  Loss: 0.0573\n",
      "Epoch: 29/100  Loss: 0.0713\n",
      "Epoch: 30/100  Loss: 0.0558\n",
      "Epoch: 30/100  Loss: 0.0444\n",
      "Epoch: 30/100  Loss: 0.0441\n",
      "Epoch: 30/100  Loss: 0.0463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30/100  Loss: 0.0539\n",
      "Epoch: 30/100  Loss: 0.0541\n",
      "Epoch: 30/100  Loss: 0.0721\n",
      "Epoch: 30/100  Loss: 0.0620\n",
      "Epoch: 30/100  Loss: 0.0460\n",
      "Epoch: 30/100  Loss: 0.0642\n",
      "Epoch: 31/100  Loss: 0.0368\n",
      "Epoch: 31/100  Loss: 0.0325\n",
      "Epoch: 31/100  Loss: 0.0494\n",
      "Epoch: 31/100  Loss: 0.0501\n",
      "Epoch: 31/100  Loss: 0.0376\n",
      "Epoch: 31/100  Loss: 0.0306\n",
      "Epoch: 31/100  Loss: 0.0572\n",
      "Epoch: 31/100  Loss: 0.0656\n",
      "Epoch: 31/100  Loss: 0.0517\n",
      "Epoch: 31/100  Loss: 0.0620\n",
      "Epoch: 32/100  Loss: 0.0459\n",
      "Epoch: 32/100  Loss: 0.0411\n",
      "Epoch: 32/100  Loss: 0.0405\n",
      "Epoch: 32/100  Loss: 0.0438\n",
      "Epoch: 32/100  Loss: 0.0533\n",
      "Epoch: 32/100  Loss: 0.0561\n",
      "Epoch: 32/100  Loss: 0.0442\n",
      "Epoch: 32/100  Loss: 0.0538\n",
      "Epoch: 32/100  Loss: 0.0439\n",
      "Epoch: 32/100  Loss: 0.0438\n",
      "Epoch: 33/100  Loss: 0.0348\n",
      "Epoch: 33/100  Loss: 0.0367\n",
      "Epoch: 33/100  Loss: 0.0325\n",
      "Epoch: 33/100  Loss: 0.0382\n",
      "Epoch: 33/100  Loss: 0.0346\n",
      "Epoch: 33/100  Loss: 0.0615\n",
      "Epoch: 33/100  Loss: 0.0681\n",
      "Epoch: 33/100  Loss: 0.0612\n",
      "Epoch: 33/100  Loss: 0.0610\n",
      "Epoch: 33/100  Loss: 0.0546\n",
      "Epoch: 34/100  Loss: 0.0337\n",
      "Epoch: 34/100  Loss: 0.0441\n",
      "Epoch: 34/100  Loss: 0.0300\n",
      "Epoch: 34/100  Loss: 0.0364\n",
      "Epoch: 34/100  Loss: 0.0347\n",
      "Epoch: 34/100  Loss: 0.0313\n",
      "Epoch: 34/100  Loss: 0.0352\n",
      "Epoch: 34/100  Loss: 0.0559\n",
      "Epoch: 34/100  Loss: 0.0661\n",
      "Epoch: 34/100  Loss: 0.0500\n",
      "Epoch: 35/100  Loss: 0.0390\n",
      "Epoch: 35/100  Loss: 0.0455\n",
      "Epoch: 35/100  Loss: 0.0431\n",
      "Epoch: 35/100  Loss: 0.0422\n",
      "Epoch: 35/100  Loss: 0.0319\n",
      "Epoch: 35/100  Loss: 0.0532\n",
      "Epoch: 35/100  Loss: 0.0434\n",
      "Epoch: 35/100  Loss: 0.0368\n",
      "Epoch: 35/100  Loss: 0.0437\n",
      "Epoch: 35/100  Loss: 0.0552\n",
      "Epoch: 36/100  Loss: 0.0363\n",
      "Epoch: 36/100  Loss: 0.0415\n",
      "Epoch: 36/100  Loss: 0.0393\n",
      "Epoch: 36/100  Loss: 0.0348\n",
      "Epoch: 36/100  Loss: 0.0516\n",
      "Epoch: 36/100  Loss: 0.0559\n",
      "Epoch: 36/100  Loss: 0.0472\n",
      "Epoch: 36/100  Loss: 0.0692\n",
      "Epoch: 36/100  Loss: 0.0555\n",
      "Epoch: 36/100  Loss: 0.0420\n",
      "Epoch: 37/100  Loss: 0.0399\n",
      "Epoch: 37/100  Loss: 0.0337\n",
      "Epoch: 37/100  Loss: 0.0356\n",
      "Epoch: 37/100  Loss: 0.0382\n",
      "Epoch: 37/100  Loss: 0.0321\n",
      "Epoch: 37/100  Loss: 0.0321\n",
      "Epoch: 37/100  Loss: 0.0326\n",
      "Epoch: 37/100  Loss: 0.0289\n",
      "Epoch: 37/100  Loss: 0.0344\n",
      "Epoch: 37/100  Loss: 0.0424\n",
      "Epoch: 38/100  Loss: 0.0482\n",
      "Epoch: 38/100  Loss: 0.0324\n",
      "Epoch: 38/100  Loss: 0.0486\n",
      "Epoch: 38/100  Loss: 0.0460\n",
      "Epoch: 38/100  Loss: 0.0334\n",
      "Epoch: 38/100  Loss: 0.0265\n",
      "Epoch: 38/100  Loss: 0.0683\n",
      "Epoch: 38/100  Loss: 0.0586\n",
      "Epoch: 38/100  Loss: 0.0462\n",
      "Epoch: 38/100  Loss: 0.0423\n",
      "Epoch: 39/100  Loss: 0.0420\n",
      "Epoch: 39/100  Loss: 0.0379\n",
      "Epoch: 39/100  Loss: 0.0407\n",
      "Epoch: 39/100  Loss: 0.0340\n",
      "Epoch: 39/100  Loss: 0.0258\n",
      "Epoch: 39/100  Loss: 0.0487\n",
      "Epoch: 39/100  Loss: 0.0365\n",
      "Epoch: 39/100  Loss: 0.0419\n",
      "Epoch: 39/100  Loss: 0.0494\n",
      "Epoch: 39/100  Loss: 0.0452\n",
      "Epoch: 40/100  Loss: 0.0327\n",
      "Epoch: 40/100  Loss: 0.0302\n",
      "Epoch: 40/100  Loss: 0.0328\n",
      "Epoch: 40/100  Loss: 0.0374\n",
      "Epoch: 40/100  Loss: 0.0312\n",
      "Epoch: 40/100  Loss: 0.0309\n",
      "Epoch: 40/100  Loss: 0.0203\n",
      "Epoch: 40/100  Loss: 0.0342\n",
      "Epoch: 40/100  Loss: 0.0226\n",
      "Epoch: 40/100  Loss: 0.0568\n",
      "Epoch: 41/100  Loss: 0.0453\n",
      "Epoch: 41/100  Loss: 0.0433\n",
      "Epoch: 41/100  Loss: 0.0548\n",
      "Epoch: 41/100  Loss: 0.0327\n",
      "Epoch: 41/100  Loss: 0.0322\n",
      "Epoch: 41/100  Loss: 0.0336\n",
      "Epoch: 41/100  Loss: 0.0440\n",
      "Epoch: 41/100  Loss: 0.0445\n",
      "Epoch: 41/100  Loss: 0.0337\n",
      "Epoch: 41/100  Loss: 0.0370\n",
      "Epoch: 42/100  Loss: 0.0235\n",
      "Epoch: 42/100  Loss: 0.0318\n",
      "Epoch: 42/100  Loss: 0.0467\n",
      "Epoch: 42/100  Loss: 0.0423\n",
      "Epoch: 42/100  Loss: 0.0540\n",
      "Epoch: 42/100  Loss: 0.0327\n",
      "Epoch: 42/100  Loss: 0.0325\n",
      "Epoch: 42/100  Loss: 0.0464\n",
      "Epoch: 42/100  Loss: 0.0576\n",
      "Epoch: 42/100  Loss: 0.0385\n",
      "Epoch: 43/100  Loss: 0.0208\n",
      "Epoch: 43/100  Loss: 0.0297\n",
      "Epoch: 43/100  Loss: 0.0283\n",
      "Epoch: 43/100  Loss: 0.0341\n",
      "Epoch: 43/100  Loss: 0.0256\n",
      "Epoch: 43/100  Loss: 0.0388\n",
      "Epoch: 43/100  Loss: 0.0340\n",
      "Epoch: 43/100  Loss: 0.0324\n",
      "Epoch: 43/100  Loss: 0.0399\n",
      "Epoch: 43/100  Loss: 0.0381\n",
      "Epoch: 44/100  Loss: 0.0281\n",
      "Epoch: 44/100  Loss: 0.0407\n",
      "Epoch: 44/100  Loss: 0.0215\n",
      "Epoch: 44/100  Loss: 0.0309\n",
      "Epoch: 44/100  Loss: 0.0451\n",
      "Epoch: 44/100  Loss: 0.0306\n",
      "Epoch: 44/100  Loss: 0.0430\n",
      "Epoch: 44/100  Loss: 0.0433\n",
      "Epoch: 44/100  Loss: 0.0400\n",
      "Epoch: 44/100  Loss: 0.0537\n",
      "Epoch: 45/100  Loss: 0.0462\n",
      "Epoch: 45/100  Loss: 0.0338\n",
      "Epoch: 45/100  Loss: 0.0221\n",
      "Epoch: 45/100  Loss: 0.0290\n",
      "Epoch: 45/100  Loss: 0.0444\n",
      "Epoch: 45/100  Loss: 0.0306\n",
      "Epoch: 45/100  Loss: 0.0360\n",
      "Epoch: 45/100  Loss: 0.0355\n",
      "Epoch: 45/100  Loss: 0.0279\n",
      "Epoch: 45/100  Loss: 0.0343\n",
      "Epoch: 46/100  Loss: 0.0181\n",
      "Epoch: 46/100  Loss: 0.0284\n",
      "Epoch: 46/100  Loss: 0.0300\n",
      "Epoch: 46/100  Loss: 0.0362\n",
      "Epoch: 46/100  Loss: 0.0357\n",
      "Epoch: 46/100  Loss: 0.0303\n",
      "Epoch: 46/100  Loss: 0.0397\n",
      "Epoch: 46/100  Loss: 0.0286\n",
      "Epoch: 46/100  Loss: 0.0387\n",
      "Epoch: 46/100  Loss: 0.0322\n",
      "Epoch: 47/100  Loss: 0.0384\n",
      "Epoch: 47/100  Loss: 0.0300\n",
      "Epoch: 47/100  Loss: 0.0244\n",
      "Epoch: 47/100  Loss: 0.0220\n",
      "Epoch: 47/100  Loss: 0.0261\n",
      "Epoch: 47/100  Loss: 0.0246\n",
      "Epoch: 47/100  Loss: 0.0410\n",
      "Epoch: 47/100  Loss: 0.0302\n",
      "Epoch: 47/100  Loss: 0.0397\n",
      "Epoch: 47/100  Loss: 0.0477\n",
      "Epoch: 48/100  Loss: 0.0246\n",
      "Epoch: 48/100  Loss: 0.0241\n",
      "Epoch: 48/100  Loss: 0.0345\n",
      "Epoch: 48/100  Loss: 0.0214\n",
      "Epoch: 48/100  Loss: 0.0298\n",
      "Epoch: 48/100  Loss: 0.0250\n",
      "Epoch: 48/100  Loss: 0.0301\n",
      "Epoch: 48/100  Loss: 0.0361\n",
      "Epoch: 48/100  Loss: 0.0564\n",
      "Epoch: 48/100  Loss: 0.0339\n",
      "Epoch: 49/100  Loss: 0.0327\n",
      "Epoch: 49/100  Loss: 0.0187\n",
      "Epoch: 49/100  Loss: 0.0235\n",
      "Epoch: 49/100  Loss: 0.0325\n",
      "Epoch: 49/100  Loss: 0.0399\n",
      "Epoch: 49/100  Loss: 0.0312\n",
      "Epoch: 49/100  Loss: 0.0456\n",
      "Epoch: 49/100  Loss: 0.0440\n",
      "Epoch: 49/100  Loss: 0.0437\n",
      "Epoch: 49/100  Loss: 0.0387\n",
      "Epoch: 50/100  Loss: 0.0169\n",
      "Epoch: 50/100  Loss: 0.0264\n",
      "Epoch: 50/100  Loss: 0.0203\n",
      "Epoch: 50/100  Loss: 0.0288\n",
      "Epoch: 50/100  Loss: 0.0316\n",
      "Epoch: 50/100  Loss: 0.0210\n",
      "Epoch: 50/100  Loss: 0.0284\n",
      "Epoch: 50/100  Loss: 0.0300\n",
      "Epoch: 50/100  Loss: 0.0229\n",
      "Epoch: 50/100  Loss: 0.0225\n",
      "Epoch: 51/100  Loss: 0.0203\n",
      "Epoch: 51/100  Loss: 0.0234\n",
      "Epoch: 51/100  Loss: 0.0248\n",
      "Epoch: 51/100  Loss: 0.0192\n",
      "Epoch: 51/100  Loss: 0.0126\n",
      "Epoch: 51/100  Loss: 0.0230\n",
      "Epoch: 51/100  Loss: 0.0383\n",
      "Epoch: 51/100  Loss: 0.0264\n",
      "Epoch: 51/100  Loss: 0.0335\n",
      "Epoch: 51/100  Loss: 0.0351\n",
      "Epoch: 52/100  Loss: 0.0396\n",
      "Epoch: 52/100  Loss: 0.0341\n",
      "Epoch: 52/100  Loss: 0.0362\n",
      "Epoch: 52/100  Loss: 0.0225\n",
      "Epoch: 52/100  Loss: 0.0393\n",
      "Epoch: 52/100  Loss: 0.0210\n",
      "Epoch: 52/100  Loss: 0.0321\n",
      "Epoch: 52/100  Loss: 0.0306\n",
      "Epoch: 52/100  Loss: 0.0286\n",
      "Epoch: 52/100  Loss: 0.0314\n",
      "Epoch: 53/100  Loss: 0.0231\n",
      "Epoch: 53/100  Loss: 0.0371\n",
      "Epoch: 53/100  Loss: 0.0199\n",
      "Epoch: 53/100  Loss: 0.0228\n",
      "Epoch: 53/100  Loss: 0.0284\n",
      "Epoch: 53/100  Loss: 0.0313\n",
      "Epoch: 53/100  Loss: 0.0314\n",
      "Epoch: 53/100  Loss: 0.0289\n",
      "Epoch: 53/100  Loss: 0.0293\n",
      "Epoch: 53/100  Loss: 0.0266\n",
      "Epoch: 54/100  Loss: 0.0233\n",
      "Epoch: 54/100  Loss: 0.0275\n",
      "Epoch: 54/100  Loss: 0.0281\n",
      "Epoch: 54/100  Loss: 0.0323\n",
      "Epoch: 54/100  Loss: 0.0317\n",
      "Epoch: 54/100  Loss: 0.0293\n",
      "Epoch: 54/100  Loss: 0.0408\n",
      "Epoch: 54/100  Loss: 0.0275\n",
      "Epoch: 54/100  Loss: 0.0163\n",
      "Epoch: 54/100  Loss: 0.0289\n",
      "Epoch: 55/100  Loss: 0.0214\n",
      "Epoch: 55/100  Loss: 0.0245\n",
      "Epoch: 55/100  Loss: 0.0214\n",
      "Epoch: 55/100  Loss: 0.0307\n",
      "Epoch: 55/100  Loss: 0.0332\n",
      "Epoch: 55/100  Loss: 0.0343\n",
      "Epoch: 55/100  Loss: 0.0261\n",
      "Epoch: 55/100  Loss: 0.0307\n",
      "Epoch: 55/100  Loss: 0.0324\n",
      "Epoch: 55/100  Loss: 0.0338\n",
      "Epoch: 56/100  Loss: 0.0202\n",
      "Epoch: 56/100  Loss: 0.0226\n",
      "Epoch: 56/100  Loss: 0.0178\n",
      "Epoch: 56/100  Loss: 0.0309\n",
      "Epoch: 56/100  Loss: 0.0179\n",
      "Epoch: 56/100  Loss: 0.0194\n",
      "Epoch: 56/100  Loss: 0.0257\n",
      "Epoch: 56/100  Loss: 0.0389\n",
      "Epoch: 56/100  Loss: 0.0463\n",
      "Epoch: 56/100  Loss: 0.0261\n",
      "Epoch: 57/100  Loss: 0.0149\n",
      "Epoch: 57/100  Loss: 0.0172\n",
      "Epoch: 57/100  Loss: 0.0211\n",
      "Epoch: 57/100  Loss: 0.0316\n",
      "Epoch: 57/100  Loss: 0.0205\n",
      "Epoch: 57/100  Loss: 0.0312\n",
      "Epoch: 57/100  Loss: 0.0340\n",
      "Epoch: 57/100  Loss: 0.0204\n",
      "Epoch: 57/100  Loss: 0.0303\n",
      "Epoch: 57/100  Loss: 0.0257\n",
      "Epoch: 58/100  Loss: 0.0162\n",
      "Epoch: 58/100  Loss: 0.0123\n",
      "Epoch: 58/100  Loss: 0.0258\n",
      "Epoch: 58/100  Loss: 0.0149\n",
      "Epoch: 58/100  Loss: 0.0277\n",
      "Epoch: 58/100  Loss: 0.0528\n",
      "Epoch: 58/100  Loss: 0.0419\n",
      "Epoch: 58/100  Loss: 0.0263\n",
      "Epoch: 58/100  Loss: 0.0407\n",
      "Epoch: 58/100  Loss: 0.0398\n",
      "Epoch: 59/100  Loss: 0.0253\n",
      "Epoch: 59/100  Loss: 0.0300\n",
      "Epoch: 59/100  Loss: 0.0233\n",
      "Epoch: 59/100  Loss: 0.0164\n",
      "Epoch: 59/100  Loss: 0.0247\n",
      "Epoch: 59/100  Loss: 0.0404\n",
      "Epoch: 59/100  Loss: 0.0353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 59/100  Loss: 0.0243\n",
      "Epoch: 59/100  Loss: 0.0380\n",
      "Epoch: 59/100  Loss: 0.0252\n",
      "Epoch: 60/100  Loss: 0.0122\n",
      "Epoch: 60/100  Loss: 0.0169\n",
      "Epoch: 60/100  Loss: 0.0132\n",
      "Epoch: 60/100  Loss: 0.0194\n",
      "Epoch: 60/100  Loss: 0.0286\n",
      "Epoch: 60/100  Loss: 0.0340\n",
      "Epoch: 60/100  Loss: 0.0184\n",
      "Epoch: 60/100  Loss: 0.0170\n",
      "Epoch: 60/100  Loss: 0.0248\n",
      "Epoch: 60/100  Loss: 0.0225\n",
      "Epoch: 61/100  Loss: 0.0190\n",
      "Epoch: 61/100  Loss: 0.0131\n",
      "Epoch: 61/100  Loss: 0.0168\n",
      "Epoch: 61/100  Loss: 0.0268\n",
      "Epoch: 61/100  Loss: 0.0275\n",
      "Epoch: 61/100  Loss: 0.0174\n",
      "Epoch: 61/100  Loss: 0.0352\n",
      "Epoch: 61/100  Loss: 0.0370\n",
      "Epoch: 61/100  Loss: 0.0280\n",
      "Epoch: 61/100  Loss: 0.0420\n",
      "Epoch: 62/100  Loss: 0.0247\n",
      "Epoch: 62/100  Loss: 0.0349\n",
      "Epoch: 62/100  Loss: 0.0292\n",
      "Epoch: 62/100  Loss: 0.0256\n",
      "Epoch: 62/100  Loss: 0.0335\n",
      "Epoch: 62/100  Loss: 0.0358\n",
      "Epoch: 62/100  Loss: 0.0219\n",
      "Epoch: 62/100  Loss: 0.0155\n",
      "Epoch: 62/100  Loss: 0.0214\n",
      "Epoch: 62/100  Loss: 0.0172\n",
      "Epoch: 63/100  Loss: 0.0124\n",
      "Epoch: 63/100  Loss: 0.0122\n",
      "Epoch: 63/100  Loss: 0.0180\n",
      "Epoch: 63/100  Loss: 0.0102\n",
      "Epoch: 63/100  Loss: 0.0231\n",
      "Epoch: 63/100  Loss: 0.0103\n",
      "Epoch: 63/100  Loss: 0.0087\n",
      "Epoch: 63/100  Loss: 0.0150\n",
      "Epoch: 63/100  Loss: 0.0190\n",
      "Epoch: 63/100  Loss: 0.0234\n",
      "Epoch: 64/100  Loss: 0.0116\n",
      "Epoch: 64/100  Loss: 0.0149\n",
      "Epoch: 64/100  Loss: 0.0222\n",
      "Epoch: 64/100  Loss: 0.0200\n",
      "Epoch: 64/100  Loss: 0.0363\n",
      "Epoch: 64/100  Loss: 0.0333\n",
      "Epoch: 64/100  Loss: 0.0356\n",
      "Epoch: 64/100  Loss: 0.0369\n",
      "Epoch: 64/100  Loss: 0.0242\n",
      "Epoch: 64/100  Loss: 0.0356\n",
      "Epoch: 65/100  Loss: 0.0106\n",
      "Epoch: 65/100  Loss: 0.0283\n",
      "Epoch: 65/100  Loss: 0.0252\n",
      "Epoch: 65/100  Loss: 0.0253\n",
      "Epoch: 65/100  Loss: 0.0298\n",
      "Epoch: 65/100  Loss: 0.0412\n",
      "Epoch: 65/100  Loss: 0.0340\n",
      "Epoch: 65/100  Loss: 0.0241\n",
      "Epoch: 65/100  Loss: 0.0198\n",
      "Epoch: 65/100  Loss: 0.0240\n",
      "Epoch: 66/100  Loss: 0.0233\n",
      "Epoch: 66/100  Loss: 0.0143\n",
      "Epoch: 66/100  Loss: 0.0229\n",
      "Epoch: 66/100  Loss: 0.0220\n",
      "Epoch: 66/100  Loss: 0.0193\n",
      "Epoch: 66/100  Loss: 0.0155\n",
      "Epoch: 66/100  Loss: 0.0220\n",
      "Epoch: 66/100  Loss: 0.0214\n",
      "Epoch: 66/100  Loss: 0.0238\n",
      "Epoch: 66/100  Loss: 0.0316\n",
      "Epoch: 67/100  Loss: 0.0203\n",
      "Epoch: 67/100  Loss: 0.0228\n",
      "Epoch: 67/100  Loss: 0.0159\n",
      "Epoch: 67/100  Loss: 0.0133\n",
      "Epoch: 67/100  Loss: 0.0103\n",
      "Epoch: 67/100  Loss: 0.0207\n",
      "Epoch: 67/100  Loss: 0.0324\n",
      "Epoch: 67/100  Loss: 0.0206\n",
      "Epoch: 67/100  Loss: 0.0315\n",
      "Epoch: 67/100  Loss: 0.0237\n",
      "Epoch: 68/100  Loss: 0.0148\n",
      "Epoch: 68/100  Loss: 0.0119\n",
      "Epoch: 68/100  Loss: 0.0282\n",
      "Epoch: 68/100  Loss: 0.0277\n",
      "Epoch: 68/100  Loss: 0.0273\n",
      "Epoch: 68/100  Loss: 0.0504\n",
      "Epoch: 68/100  Loss: 0.0212\n",
      "Epoch: 68/100  Loss: 0.0230\n",
      "Epoch: 68/100  Loss: 0.0163\n",
      "Epoch: 68/100  Loss: 0.0222\n",
      "Epoch: 69/100  Loss: 0.0211\n",
      "Epoch: 69/100  Loss: 0.0312\n",
      "Epoch: 69/100  Loss: 0.0222\n",
      "Epoch: 69/100  Loss: 0.0263\n",
      "Epoch: 69/100  Loss: 0.0291\n",
      "Epoch: 69/100  Loss: 0.0332\n",
      "Epoch: 69/100  Loss: 0.0214\n",
      "Epoch: 69/100  Loss: 0.0236\n",
      "Epoch: 69/100  Loss: 0.0174\n",
      "Epoch: 69/100  Loss: 0.0117\n",
      "Epoch: 70/100  Loss: 0.0113\n",
      "Epoch: 70/100  Loss: 0.0217\n",
      "Epoch: 70/100  Loss: 0.0094\n",
      "Epoch: 70/100  Loss: 0.0190\n",
      "Epoch: 70/100  Loss: 0.0170\n",
      "Epoch: 70/100  Loss: 0.0092\n",
      "Epoch: 70/100  Loss: 0.0221\n",
      "Epoch: 70/100  Loss: 0.0190\n",
      "Epoch: 70/100  Loss: 0.0287\n",
      "Epoch: 70/100  Loss: 0.0219\n",
      "Epoch: 71/100  Loss: 0.0235\n",
      "Epoch: 71/100  Loss: 0.0090\n",
      "Epoch: 71/100  Loss: 0.0323\n",
      "Epoch: 71/100  Loss: 0.0223\n",
      "Epoch: 71/100  Loss: 0.0320\n",
      "Epoch: 71/100  Loss: 0.0294\n",
      "Epoch: 71/100  Loss: 0.0312\n",
      "Epoch: 71/100  Loss: 0.0225\n",
      "Epoch: 71/100  Loss: 0.0225\n",
      "Epoch: 71/100  Loss: 0.0138\n",
      "Epoch: 72/100  Loss: 0.0235\n",
      "Epoch: 72/100  Loss: 0.0168\n",
      "Epoch: 72/100  Loss: 0.0263\n",
      "Epoch: 72/100  Loss: 0.0234\n",
      "Epoch: 72/100  Loss: 0.0193\n",
      "Epoch: 72/100  Loss: 0.0284\n",
      "Epoch: 72/100  Loss: 0.0173\n",
      "Epoch: 72/100  Loss: 0.0208\n",
      "Epoch: 72/100  Loss: 0.0118\n",
      "Epoch: 72/100  Loss: 0.0298\n",
      "Epoch: 73/100  Loss: 0.0233\n",
      "Epoch: 73/100  Loss: 0.0084\n",
      "Epoch: 73/100  Loss: 0.0218\n",
      "Epoch: 73/100  Loss: 0.0140\n",
      "Epoch: 73/100  Loss: 0.0166\n",
      "Epoch: 73/100  Loss: 0.0241\n",
      "Epoch: 73/100  Loss: 0.0176\n",
      "Epoch: 73/100  Loss: 0.0275\n",
      "Epoch: 73/100  Loss: 0.0261\n",
      "Epoch: 73/100  Loss: 0.0257\n",
      "Epoch: 74/100  Loss: 0.0181\n",
      "Epoch: 74/100  Loss: 0.0138\n",
      "Epoch: 74/100  Loss: 0.0131\n",
      "Epoch: 74/100  Loss: 0.0298\n",
      "Epoch: 74/100  Loss: 0.0302\n",
      "Epoch: 74/100  Loss: 0.0207\n",
      "Epoch: 74/100  Loss: 0.0148\n",
      "Epoch: 74/100  Loss: 0.0178\n",
      "Epoch: 74/100  Loss: 0.0185\n",
      "Epoch: 74/100  Loss: 0.0287\n",
      "Epoch: 75/100  Loss: 0.0230\n",
      "Epoch: 75/100  Loss: 0.0268\n",
      "Epoch: 75/100  Loss: 0.0213\n",
      "Epoch: 75/100  Loss: 0.0233\n",
      "Epoch: 75/100  Loss: 0.0294\n",
      "Epoch: 75/100  Loss: 0.0153\n",
      "Epoch: 75/100  Loss: 0.0145\n",
      "Epoch: 75/100  Loss: 0.0100\n",
      "Epoch: 75/100  Loss: 0.0108\n",
      "Epoch: 75/100  Loss: 0.0185\n",
      "Epoch: 76/100  Loss: 0.0235\n",
      "Epoch: 76/100  Loss: 0.0147\n",
      "Epoch: 76/100  Loss: 0.0215\n",
      "Epoch: 76/100  Loss: 0.0096\n",
      "Epoch: 76/100  Loss: 0.0152\n",
      "Epoch: 76/100  Loss: 0.0111\n",
      "Epoch: 76/100  Loss: 0.0169\n",
      "Epoch: 76/100  Loss: 0.0220\n",
      "Epoch: 76/100  Loss: 0.0283\n",
      "Epoch: 76/100  Loss: 0.0275\n",
      "Epoch: 77/100  Loss: 0.0243\n",
      "Epoch: 77/100  Loss: 0.0165\n",
      "Epoch: 77/100  Loss: 0.0236\n",
      "Epoch: 77/100  Loss: 0.0177\n",
      "Epoch: 77/100  Loss: 0.0376\n",
      "Epoch: 77/100  Loss: 0.0137\n",
      "Epoch: 77/100  Loss: 0.0221\n",
      "Epoch: 77/100  Loss: 0.0241\n",
      "Epoch: 77/100  Loss: 0.0161\n",
      "Epoch: 77/100  Loss: 0.0186\n",
      "Epoch: 78/100  Loss: 0.0117\n",
      "Epoch: 78/100  Loss: 0.0179\n",
      "Epoch: 78/100  Loss: 0.0154\n",
      "Epoch: 78/100  Loss: 0.0215\n",
      "Epoch: 78/100  Loss: 0.0165\n",
      "Epoch: 78/100  Loss: 0.0217\n",
      "Epoch: 78/100  Loss: 0.0190\n",
      "Epoch: 78/100  Loss: 0.0254\n",
      "Epoch: 78/100  Loss: 0.0234\n",
      "Epoch: 78/100  Loss: 0.0175\n",
      "Epoch: 79/100  Loss: 0.0186\n",
      "Epoch: 79/100  Loss: 0.0257\n",
      "Epoch: 79/100  Loss: 0.0149\n",
      "Epoch: 79/100  Loss: 0.0226\n",
      "Epoch: 79/100  Loss: 0.0181\n",
      "Epoch: 79/100  Loss: 0.0227\n",
      "Epoch: 79/100  Loss: 0.0236\n",
      "Epoch: 79/100  Loss: 0.0168\n",
      "Epoch: 79/100  Loss: 0.0270\n",
      "Epoch: 79/100  Loss: 0.0132\n",
      "Epoch: 80/100  Loss: 0.0154\n",
      "Epoch: 80/100  Loss: 0.0132\n",
      "Epoch: 80/100  Loss: 0.0177\n",
      "Epoch: 80/100  Loss: 0.0232\n",
      "Epoch: 80/100  Loss: 0.0235\n",
      "Epoch: 80/100  Loss: 0.0223\n",
      "Epoch: 80/100  Loss: 0.0182\n",
      "Epoch: 80/100  Loss: 0.0223\n",
      "Epoch: 80/100  Loss: 0.0195\n",
      "Epoch: 80/100  Loss: 0.0408\n",
      "Epoch: 81/100  Loss: 0.0153\n",
      "Epoch: 81/100  Loss: 0.0142\n",
      "Epoch: 81/100  Loss: 0.0158\n",
      "Epoch: 81/100  Loss: 0.0181\n",
      "Epoch: 81/100  Loss: 0.0173\n",
      "Epoch: 81/100  Loss: 0.0207\n",
      "Epoch: 81/100  Loss: 0.0200\n",
      "Epoch: 81/100  Loss: 0.0176\n",
      "Epoch: 81/100  Loss: 0.0178\n",
      "Epoch: 81/100  Loss: 0.0144\n",
      "Epoch: 82/100  Loss: 0.0166\n",
      "Epoch: 82/100  Loss: 0.0228\n",
      "Epoch: 82/100  Loss: 0.0149\n",
      "Epoch: 82/100  Loss: 0.0155\n",
      "Epoch: 82/100  Loss: 0.0227\n",
      "Epoch: 82/100  Loss: 0.0206\n",
      "Epoch: 82/100  Loss: 0.0207\n",
      "Epoch: 82/100  Loss: 0.0344\n",
      "Epoch: 82/100  Loss: 0.0205\n",
      "Epoch: 82/100  Loss: 0.0303\n",
      "Epoch: 83/100  Loss: 0.0106\n",
      "Epoch: 83/100  Loss: 0.0108\n",
      "Epoch: 83/100  Loss: 0.0220\n",
      "Epoch: 83/100  Loss: 0.0112\n",
      "Epoch: 83/100  Loss: 0.0170\n",
      "Epoch: 83/100  Loss: 0.0129\n",
      "Epoch: 83/100  Loss: 0.0076\n",
      "Epoch: 83/100  Loss: 0.0203\n",
      "Epoch: 83/100  Loss: 0.0121\n",
      "Epoch: 83/100  Loss: 0.0139\n",
      "Epoch: 84/100  Loss: 0.0061\n",
      "Epoch: 84/100  Loss: 0.0067\n",
      "Epoch: 84/100  Loss: 0.0080\n",
      "Epoch: 84/100  Loss: 0.0189\n",
      "Epoch: 84/100  Loss: 0.0185\n",
      "Epoch: 84/100  Loss: 0.0174\n",
      "Epoch: 84/100  Loss: 0.0134\n",
      "Epoch: 84/100  Loss: 0.0180\n",
      "Epoch: 84/100  Loss: 0.0183\n",
      "Epoch: 84/100  Loss: 0.0212\n",
      "Epoch: 85/100  Loss: 0.0229\n",
      "Epoch: 85/100  Loss: 0.0175\n",
      "Epoch: 85/100  Loss: 0.0195\n",
      "Epoch: 85/100  Loss: 0.0216\n",
      "Epoch: 85/100  Loss: 0.0238\n",
      "Epoch: 85/100  Loss: 0.0320\n",
      "Epoch: 85/100  Loss: 0.0196\n",
      "Epoch: 85/100  Loss: 0.0258\n",
      "Epoch: 85/100  Loss: 0.0371\n",
      "Epoch: 85/100  Loss: 0.0120\n",
      "Epoch: 86/100  Loss: 0.0095\n",
      "Epoch: 86/100  Loss: 0.0145\n",
      "Epoch: 86/100  Loss: 0.0167\n",
      "Epoch: 86/100  Loss: 0.0116\n",
      "Epoch: 86/100  Loss: 0.0168\n",
      "Epoch: 86/100  Loss: 0.0136\n",
      "Epoch: 86/100  Loss: 0.0206\n",
      "Epoch: 86/100  Loss: 0.0232\n",
      "Epoch: 86/100  Loss: 0.0291\n",
      "Epoch: 86/100  Loss: 0.0337\n",
      "Epoch: 87/100  Loss: 0.0275\n",
      "Epoch: 87/100  Loss: 0.0128\n",
      "Epoch: 87/100  Loss: 0.0146\n",
      "Epoch: 87/100  Loss: 0.0166\n",
      "Epoch: 87/100  Loss: 0.0281\n",
      "Epoch: 87/100  Loss: 0.0252\n",
      "Epoch: 87/100  Loss: 0.0135\n",
      "Epoch: 87/100  Loss: 0.0162\n",
      "Epoch: 87/100  Loss: 0.0179\n",
      "Epoch: 87/100  Loss: 0.0194\n",
      "Epoch: 88/100  Loss: 0.0123\n",
      "Epoch: 88/100  Loss: 0.0134\n",
      "Epoch: 88/100  Loss: 0.0118\n",
      "Epoch: 88/100  Loss: 0.0145\n",
      "Epoch: 88/100  Loss: 0.0150\n",
      "Epoch: 88/100  Loss: 0.0183\n",
      "Epoch: 88/100  Loss: 0.0151\n",
      "Epoch: 88/100  Loss: 0.0139\n",
      "Epoch: 88/100  Loss: 0.0180\n",
      "Epoch: 88/100  Loss: 0.0213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 89/100  Loss: 0.0229\n",
      "Epoch: 89/100  Loss: 0.0150\n",
      "Epoch: 89/100  Loss: 0.0072\n",
      "Epoch: 89/100  Loss: 0.0111\n",
      "Epoch: 89/100  Loss: 0.0127\n",
      "Epoch: 89/100  Loss: 0.0269\n",
      "Epoch: 89/100  Loss: 0.0263\n",
      "Epoch: 89/100  Loss: 0.0199\n",
      "Epoch: 89/100  Loss: 0.0137\n",
      "Epoch: 89/100  Loss: 0.0192\n",
      "Epoch: 90/100  Loss: 0.0333\n",
      "Epoch: 90/100  Loss: 0.0108\n",
      "Epoch: 90/100  Loss: 0.0130\n",
      "Epoch: 90/100  Loss: 0.0065\n",
      "Epoch: 90/100  Loss: 0.0123\n",
      "Epoch: 90/100  Loss: 0.0093\n",
      "Epoch: 90/100  Loss: 0.0104\n",
      "Epoch: 90/100  Loss: 0.0234\n",
      "Epoch: 90/100  Loss: 0.0245\n",
      "Epoch: 90/100  Loss: 0.0224\n",
      "Epoch: 91/100  Loss: 0.0129\n",
      "Epoch: 91/100  Loss: 0.0096\n",
      "Epoch: 91/100  Loss: 0.0195\n",
      "Epoch: 91/100  Loss: 0.0089\n",
      "Epoch: 91/100  Loss: 0.0435\n",
      "Epoch: 91/100  Loss: 0.0207\n",
      "Epoch: 91/100  Loss: 0.0306\n",
      "Epoch: 91/100  Loss: 0.0135\n",
      "Epoch: 91/100  Loss: 0.0230\n",
      "Epoch: 91/100  Loss: 0.0172\n",
      "Epoch: 92/100  Loss: 0.0144\n",
      "Epoch: 92/100  Loss: 0.0119\n",
      "Epoch: 92/100  Loss: 0.0135\n",
      "Epoch: 92/100  Loss: 0.0216\n",
      "Epoch: 92/100  Loss: 0.0138\n",
      "Epoch: 92/100  Loss: 0.0280\n",
      "Epoch: 92/100  Loss: 0.0215\n",
      "Epoch: 92/100  Loss: 0.0139\n",
      "Epoch: 92/100  Loss: 0.0213\n",
      "Epoch: 92/100  Loss: 0.0183\n",
      "Epoch: 93/100  Loss: 0.0189\n",
      "Epoch: 93/100  Loss: 0.0270\n",
      "Epoch: 93/100  Loss: 0.0238\n",
      "Epoch: 93/100  Loss: 0.0124\n",
      "Epoch: 93/100  Loss: 0.0130\n",
      "Epoch: 93/100  Loss: 0.0147\n",
      "Epoch: 93/100  Loss: 0.0298\n",
      "Epoch: 93/100  Loss: 0.0139\n",
      "Epoch: 93/100  Loss: 0.0176\n",
      "Epoch: 93/100  Loss: 0.0115\n",
      "Epoch: 94/100  Loss: 0.0142\n",
      "Epoch: 94/100  Loss: 0.0160\n",
      "Epoch: 94/100  Loss: 0.0168\n",
      "Epoch: 94/100  Loss: 0.0129\n",
      "Epoch: 94/100  Loss: 0.0101\n",
      "Epoch: 94/100  Loss: 0.0256\n",
      "Epoch: 94/100  Loss: 0.0194\n",
      "Epoch: 94/100  Loss: 0.0167\n",
      "Epoch: 94/100  Loss: 0.0145\n",
      "Epoch: 94/100  Loss: 0.0195\n",
      "Epoch: 95/100  Loss: 0.0114\n",
      "Epoch: 95/100  Loss: 0.0076\n",
      "Epoch: 95/100  Loss: 0.0091\n",
      "Epoch: 95/100  Loss: 0.0087\n",
      "Epoch: 95/100  Loss: 0.0108\n",
      "Epoch: 95/100  Loss: 0.0211\n",
      "Epoch: 95/100  Loss: 0.0202\n",
      "Epoch: 95/100  Loss: 0.0207\n",
      "Epoch: 95/100  Loss: 0.0150\n",
      "Epoch: 95/100  Loss: 0.0152\n",
      "Epoch: 96/100  Loss: 0.0112\n",
      "Epoch: 96/100  Loss: 0.0166\n",
      "Epoch: 96/100  Loss: 0.0288\n",
      "Epoch: 96/100  Loss: 0.0222\n",
      "Epoch: 96/100  Loss: 0.0232\n",
      "Epoch: 96/100  Loss: 0.0210\n",
      "Epoch: 96/100  Loss: 0.0193\n",
      "Epoch: 96/100  Loss: 0.0214\n",
      "Epoch: 96/100  Loss: 0.0181\n",
      "Epoch: 96/100  Loss: 0.0107\n",
      "Epoch: 97/100  Loss: 0.0132\n",
      "Epoch: 97/100  Loss: 0.0117\n",
      "Epoch: 97/100  Loss: 0.0145\n",
      "Epoch: 97/100  Loss: 0.0086\n",
      "Epoch: 97/100  Loss: 0.0241\n",
      "Epoch: 97/100  Loss: 0.0202\n",
      "Epoch: 97/100  Loss: 0.0154\n",
      "Epoch: 97/100  Loss: 0.0220\n",
      "Epoch: 97/100  Loss: 0.0198\n",
      "Epoch: 97/100  Loss: 0.0117\n",
      "Epoch: 98/100  Loss: 0.0209\n",
      "Epoch: 98/100  Loss: 0.0122\n",
      "Epoch: 98/100  Loss: 0.0093\n",
      "Epoch: 98/100  Loss: 0.0155\n",
      "Epoch: 98/100  Loss: 0.0184\n",
      "Epoch: 98/100  Loss: 0.0124\n",
      "Epoch: 98/100  Loss: 0.0087\n",
      "Epoch: 98/100  Loss: 0.0124\n",
      "Epoch: 98/100  Loss: 0.0208\n",
      "Epoch: 98/100  Loss: 0.0198\n",
      "Epoch: 99/100  Loss: 0.0068\n",
      "Epoch: 99/100  Loss: 0.0212\n",
      "Epoch: 99/100  Loss: 0.0156\n",
      "Epoch: 99/100  Loss: 0.0096\n",
      "Epoch: 99/100  Loss: 0.0235\n",
      "Epoch: 99/100  Loss: 0.0191\n",
      "Epoch: 99/100  Loss: 0.0095\n",
      "Epoch: 99/100  Loss: 0.0065\n",
      "Epoch: 99/100  Loss: 0.0201\n",
      "Epoch: 99/100  Loss: 0.0108\n",
      "Epoch: 100/100  Loss: 0.0088\n",
      "Epoch: 100/100  Loss: 0.0164\n",
      "Epoch: 100/100  Loss: 0.0143\n",
      "Epoch: 100/100  Loss: 0.0172\n",
      "Epoch: 100/100  Loss: 0.0215\n",
      "Epoch: 100/100  Loss: 0.0218\n",
      "Epoch: 100/100  Loss: 0.0292\n",
      "Epoch: 100/100  Loss: 0.0157\n",
      "Epoch: 100/100  Loss: 0.0144\n",
      "Epoch: 100/100  Loss: 0.0317\n"
     ]
    }
   ],
   "source": [
    "fit_model(model, train, train_labels, epochs = epochs, n_chunks = n_chunks, learning_rate = learning_rate, weight_decay = weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for train set is 0.9986190476190476\n",
      "Accuracy score for test set is 0.8707777777777778\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9986190476190476, 0.8707777777777778)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(model, train, train_labels, test, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '../service/model.nnet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ../service/model.nnet\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metainfo = {'input_size': input_size,\n",
    "            'output_size': output_size,\n",
    "            'hidden_layers': hidden_sizes,\n",
    "            'dropout': dropout,\n",
    "            'state_dict': model.state_dict()}\n",
    "\n",
    "torch.save(metainfo, filepath)\n",
    "print(\"Model saved to {}\\n\".format(filepath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
